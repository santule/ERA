{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santule/ERA/blob/main/S13/yolo3_gradcam_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/AI/ERA_course/session13_part3_eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYHMhibnrQNN",
        "outputId": "84c26f64-600e-4449-bb0c-e243d184c79f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/MyDrive/AI/ERA_course/session13_part3_eval\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOAD LIGHTNING MODEL"
      ],
      "metadata": {
        "id": "u_CgXmExmLWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning --quiet\n",
        "!pip install lightning-bolts --quiet"
      ],
      "metadata": {
        "id": "uUFs_aqgq-Y6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import config\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import matplotlib\n",
        "\n",
        "from model import YOLOv3\n",
        "from tqdm import tqdm\n",
        "from utils_org import (\n",
        "    mean_average_precision,\n",
        "    cells_to_bboxes,\n",
        "    get_evaluation_bboxes,\n",
        "    save_checkpoint,\n",
        "    load_checkpoint,\n",
        "    check_class_accuracy,\n",
        "    plot_couple_examples,\n",
        "    accuracy_fn,\n",
        "    get_loaders,\n",
        "    non_max_suppression,\n",
        "    plot_image\n",
        ")\n",
        "from loss import YoloLoss\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Q5SoEJgPq0Fj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9nuHbVqxqVWC"
      },
      "outputs": [],
      "source": [
        "# custom functions for yolo\n",
        "\n",
        "# loss function for yolov3\n",
        "loss_fn = YoloLoss()\n",
        "def criterion(out, y,anchors):\n",
        "  loss = (  loss_fn(out[0], y[0], anchors[0])\n",
        "            + loss_fn(out[1], y[1], anchors[1])\n",
        "            + loss_fn(out[2], y[2], anchors[2])\n",
        "            )\n",
        "  return loss\n",
        "\n",
        "# accuracy function for yolov3\n",
        "def accuracy_fn(y, out, threshold,\n",
        "                correct_class, correct_obj,\n",
        "                correct_noobj, tot_class_preds,\n",
        "                tot_obj, tot_noobj):\n",
        "\n",
        "  for i in range(3):\n",
        "\n",
        "      obj = y[i][..., 0] == 1 # in paper this is Iobj_i\n",
        "      noobj = y[i][..., 0] == 0  # in paper this is Iobj_i\n",
        "\n",
        "      correct_class += torch.sum(\n",
        "          torch.argmax(out[i][..., 5:][obj], dim=-1) == y[i][..., 5][obj]\n",
        "      )\n",
        "      tot_class_preds += torch.sum(obj)\n",
        "\n",
        "      obj_preds = torch.sigmoid(out[i][..., 0]) > threshold\n",
        "      correct_obj += torch.sum(obj_preds[obj] == y[i][..., 0][obj])\n",
        "      tot_obj += torch.sum(obj)\n",
        "      correct_noobj += torch.sum(obj_preds[noobj] == y[i][..., 0][noobj])\n",
        "      tot_noobj += torch.sum(noobj)\n",
        "\n",
        "  return((correct_class/(tot_class_preds+1e-16))*100,\n",
        "         (correct_noobj/(tot_noobj+1e-16))*100,\n",
        "         (correct_obj/(tot_obj+1e-16))*100)\n",
        "\n",
        "# pytorch lightning\n",
        "class LitYolo(LightningModule):\n",
        "    def __init__(self, num_classes=config.NUM_CLASSES, lr=config.LEARNING_RATE,weight_decay=config.WEIGHT_DECAY,threshold=config.CONF_THRESHOLD,my_dataset=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "        self.model = YOLOv3(num_classes=self.hparams.num_classes)\n",
        "        self.my_dataset = my_dataset\n",
        "        self.criterion = criterion\n",
        "        self.accuracy_fn = accuracy_fn\n",
        "        self.tot_class_preds, self.correct_class = 0, 0\n",
        "        self.tot_noobj, self.correct_noobj = 0, 0\n",
        "        self.tot_obj, self.correct_obj = 0, 0\n",
        "        self.scaled_anchors = 0\n",
        "\n",
        "    def set_scaled_anchor(self, scaled_anchors):\n",
        "      self.scaled_anchors = scaled_anchors\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = self.model(x)\n",
        "      return out\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "      # Set a new image size for the dataset at the beginning of each epoch\n",
        "      size_idx = random.choice(range(len(config.IMAGE_SIZES)))\n",
        "      self.my_dataset.set_image_size(size_idx=0)\n",
        "      self.set_scaled_anchor((\n",
        "          torch.tensor(config.ANCHORS)\n",
        "          * torch.tensor(config.S[size_idx]).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        "      ))\n",
        "\n",
        "    def on_validation_epoch_start(self):\n",
        "      self.set_scaled_anchor((\n",
        "          torch.tensor(config.ANCHORS)\n",
        "          * torch.tensor(config.S[1]).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        "      ))\n",
        "\n",
        "    def on_test_epoch_start(self):\n",
        "      self.set_scaled_anchor((\n",
        "          torch.tensor(config.ANCHORS)\n",
        "          * torch.tensor(config.S[1]).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        "      ))\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        out  = self(x)\n",
        "        loss = self.criterion(out,y,self.scaled_anchors)\n",
        "        acc  = self.accuracy_fn(y,out,self.hparams.threshold,self.correct_class,\n",
        "                                                                     self.correct_obj,\n",
        "                                                                     self.correct_noobj,\n",
        "                                                                     self.tot_class_preds,\n",
        "                                                                     self.tot_obj,\n",
        "                                                                     self.tot_noobj)\n",
        "\n",
        "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        self.log_dict({\"class_accuracy\": acc[0], \"no_object_accuracy\": acc[1], \"object_accuracy\":acc[2]},prog_bar=True,on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def evaluate(self, batch, stage=None):\n",
        "        x, y = batch\n",
        "        test_out = self(x)\n",
        "        loss = self.criterion(test_out,y,self.scaled_anchors)\n",
        "        acc  = self.accuracy_fn(y,test_out,self.hparams.threshold,self.correct_class,\n",
        "                                                                     self.correct_obj,\n",
        "                                                                     self.correct_noobj,\n",
        "                                                                     self.tot_class_preds,\n",
        "                                                                     self.tot_obj,\n",
        "                                                                     self.tot_noobj)\n",
        "\n",
        "        if stage:\n",
        "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
        "            self.log_dict({\"class_accuracy\": acc[0], \"no_object_accuracy\": acc[1], \"object_accuracy\":acc[2]},prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self.evaluate(batch, \"test\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self.evaluate(batch, \"val\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
        "        scheduler = OneCycleLR(\n",
        "                optimizer,\n",
        "                max_lr= 1E-2,\n",
        "                pct_start = 5/self.trainer.max_epochs,\n",
        "                epochs=self.trainer.max_epochs,\n",
        "                steps_per_epoch=len(train_loader),\n",
        "                div_factor=100,verbose=True,\n",
        "                three_phase=False\n",
        "            )\n",
        "        return ([optimizer],[scheduler])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRADIO APP AND GRADCAM"
      ],
      "metadata": {
        "id": "NjW4P4FN89Kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet\n",
        "!pip install albumentations --quiet\n",
        "!pip install grad-cam --quiet\n",
        "\n",
        "from pytorch_grad_cam import EigenCAM\n",
        "from pytorch_grad_cam.utils.model_targets import FasterRCNNBoxScoreTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "import albumentations as Al\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import gradio as gr\n",
        "from torchvision import transforms\n",
        "import albumentations as Al\n",
        "import utils\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2Al76CjE9Ax2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    #examples = [[\"/content/drive/MyDrive/AI/ERA_course/session13_old/PASCAL_VOC/images/009948.jpg\"],[\"/content/drive/MyDrive/AI/ERA_course/session13_old/PASCAL_VOC/images/009948.jpg\"]]\n",
        "\n",
        "    # colors for the bboxes\n",
        "    cmap = plt.get_cmap(\"tab20b\")\n",
        "    class_labels = config.PASCAL_CLASSES\n",
        "    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n",
        "    colors_hex = {class_labels[i]:matplotlib.colors.rgb2hex(colors[i]) for i in range(0,len(class_labels))}\n",
        "\n",
        "    def yolov3_reshape_transform(x): # consolidate the output from the model for gradcam to work\n",
        "      activations = []\n",
        "      size = x[0].size()[2:4] # 13 * 13\n",
        "      for x_item in x:\n",
        "        x_permute = x_item.permute(0, 1, 4, 2, 3 ) # 1,3,25,13,13\n",
        "        x_permute = x_permute.reshape((x_permute.shape[0],\n",
        "                                    x_permute.shape[1]*x_permute.shape[2],\n",
        "                                    *x_permute.shape[3:])) # 1,75,13,13\n",
        "        activations.append(torch.nn.functional.interpolate(torch.abs(x_permute), size, mode='bilinear'))\n",
        "      activations = torch.cat(activations, axis=1) # 1,255,13,13\n",
        "      return(activations)\n",
        "\n",
        "    def yolo3_inference(input_img,gradcam=True,gradcam_opa=0.5): # function for yolo inference\n",
        "\n",
        "      # load model\n",
        "      yololit = LitYolo()\n",
        "      inference_model = yololit.load_from_checkpoint(\"yolo3_model.ckpt\")\n",
        "\n",
        "      # bboxes, gradcam\n",
        "      anchors  = (torch.tensor(config.ANCHORS) * torch.tensor(config.S[1]).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2))\n",
        "      bboxes   = [[]]\n",
        "      sections = [] # to return image and annotations\n",
        "      nms_boxes_output = []\n",
        "\n",
        "      # image transformation\n",
        "      test_transforms = Al.Compose(\n",
        "        [\n",
        "            Al.LongestMaxSize(max_size=416),\n",
        "            Al.PadIfNeeded(\n",
        "                min_height=416, min_width=416, border_mode=cv2.BORDER_CONSTANT\n",
        "            ),\n",
        "            Al.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
        "        ]\n",
        "      )\n",
        "\n",
        "      input_img_copy = test_transforms(image=input_img)['image']\n",
        "      transform = transforms.ToTensor()\n",
        "      input_img_tensor = transform(input_img_copy).unsqueeze(0)\n",
        "\n",
        "      # infer the image\n",
        "      inference_model.eval()\n",
        "      test_img_out   = inference_model(input_img_tensor)\n",
        "\n",
        "      # process the outputs to create bounding boxes\n",
        "      for i in range(3):\n",
        "          batch_size, A, S, _, _ = test_img_out[i].shape # 1, anchors = 3, scaling = 13/26/52\n",
        "          anchor = anchors[i]\n",
        "          boxes_scale_i = utils.cells_to_bboxes(test_img_out[i], anchor, S=S, is_preds=True)\n",
        "          for idx, (box) in enumerate(boxes_scale_i):\n",
        "              bboxes[idx] += box\n",
        "      # nms\n",
        "      nms_boxes = utils.non_max_suppression(bboxes[0], iou_threshold=0.6, threshold=0.5, box_format=\"midpoint\",)\n",
        "      nms_boxes_output.append(nms_boxes)\n",
        "\n",
        "      # use gradio image annotations\n",
        "      height, width = 416, 416\n",
        "      for box in nms_boxes:\n",
        "        class_pred = box[0]\n",
        "        box = box[2:]\n",
        "        upper_left_x  = int((box[0] - box[2] / 2) * width)\n",
        "        upper_left_y  = max(int((box[1] - box[3] / 2) * height),0) # less than 0, box collapses\n",
        "        lower_right_x = int(upper_left_x + (box[2] * width))\n",
        "        lower_right_y = int(upper_left_y + (box[3] * height))\n",
        "        sections.append(((upper_left_x,upper_left_y,lower_right_x,lower_right_y), class_labels[int(class_pred)]))\n",
        "\n",
        "      # for gradcam\n",
        "      if gradcam:\n",
        "        objs = [b[1] for b in nms_boxes_output[0]]\n",
        "        bbox_coord = [b[2:] for b in nms_boxes_output[0]]\n",
        "        targets = [FasterRCNNBoxScoreTarget(objs, bbox_coord)]\n",
        "\n",
        "        target_layers = [inference_model.model]\n",
        "        cam = EigenCAM(inference_model, target_layers, use_cuda=False,reshape_transform=yolov3_reshape_transform)\n",
        "        grayscale_cam = cam(input_tensor = input_img_tensor, targets= targets)\n",
        "        grayscale_cam = grayscale_cam[0, :]\n",
        "        visualization = show_cam_on_image(input_img_copy, grayscale_cam, use_rgb=True, image_weight=gradcam_opa)\n",
        "\n",
        "        return (visualization,sections)\n",
        "      else:\n",
        "        return (np.array(input_img_tensor.squeeze(0).permute(1,2,0)),sections)\n",
        "\n",
        "    # app GUI\n",
        "    with gr.Row():\n",
        "        img_input  = gr.Image()\n",
        "        img_output = gr.AnnotatedImage(shape=(100, 100)).style(color_map = colors_hex)\n",
        "    with gr.Row():\n",
        "      gradcam_check = gr.Checkbox(label=\"Gradcam\")\n",
        "      gradcam_opa = gr.Slider(0, 1, value = 0.5, label=\"Opacity of GradCAM\")\n",
        "\n",
        "\n",
        "    section_btn = gr.Button(\"Identify Objects\")\n",
        "    section_btn.click(yolo3_inference, inputs=[img_input,gradcam_check,gradcam_opa], outputs=[img_output])\n",
        "    gr.Markdown(\"## Some Examples\")\n",
        "    # gr.Examples(examples=examples,\n",
        "    #                          inputs =[img_input,gradcam_check,gradcam_opa],\n",
        "    #                          outputs=img_output,\n",
        "    #                          fn=yolo3_inference, cache_examples=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "zyPfErD7_u6e",
        "outputId": "25bab827-d43a-40a1-88aa-5c2cdbfeb003"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        }
      ]
    }
  ]
}