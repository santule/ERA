{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "dlmBvQpr9zj7"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1 - GET INSTRUCT150K DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCz4GF4k94du",
    "outputId": "fb8e5342-8c7f-4c3b-92f1-67b22880008b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-25 13:27:43--  https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json\n",
      "Resolving huggingface.co (huggingface.co)... 3.160.246.13, 3.160.246.2, 3.160.246.78, ...\n",
      "Connecting to huggingface.co (huggingface.co)|3.160.246.13|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/4d/41/4d41ea1e2709f0e68e9e361e4218192b9620c5a3f2cb8055bc625942b6cd3039/6b68bc5ca2bfd8a71119af0e8454929668ccda6a334955ccc95d114fc8d082fa?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llava_instruct_150k.json%3B+filename%3D%22llava_instruct_150k.json%22%3B&response-content-type=application%2Fjson&Expires=1706448463&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjQ0ODQ2M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80ZC80MS80ZDQxZWExZTI3MDlmMGU2OGU5ZTM2MWU0MjE4MTkyYjk2MjBjNWEzZjJjYjgwNTViYzYyNTk0MmI2Y2QzMDM5LzZiNjhiYzVjYTJiZmQ4YTcxMTE5YWYwZTg0NTQ5Mjk2NjhjY2RhNmEzMzQ5NTVjY2M5NWQxMTRmYzhkMDgyZmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=FWXqsrGMzgHbB40Kkhj9aYell%7EYq7kqIWp9g0%7EHcbhShL0xvnF8HOcQ1SKy5WsUrgSt5Z07XsiwyD0L8JE1DEnunvAXA8IlMpiYMNaH%7Epq0bEQBOZuTWKL7mIgSDhbcszekCkZP55aGJAptq4BTxtglz9wmcTYco9mUkUk4Oo45MSqq8zBw8td5WhD%7E68sHIacf7iei0E4BZ5XiS8oJVYdF3wQVvE7p3hqnsDL%7EfzCnkPGNfefVuhfbccQRAB3mmkQHueL2U7PMr9eA15iaZVXsu1tA%7E-PLOW1ORGs-wlFmhu3fvai4LmKukyui22iqWVAjqLmtocwrcifq1Eo6LpQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2024-01-25 13:27:43--  https://cdn-lfs.huggingface.co/repos/4d/41/4d41ea1e2709f0e68e9e361e4218192b9620c5a3f2cb8055bc625942b6cd3039/6b68bc5ca2bfd8a71119af0e8454929668ccda6a334955ccc95d114fc8d082fa?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llava_instruct_150k.json%3B+filename%3D%22llava_instruct_150k.json%22%3B&response-content-type=application%2Fjson&Expires=1706448463&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjQ0ODQ2M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80ZC80MS80ZDQxZWExZTI3MDlmMGU2OGU5ZTM2MWU0MjE4MTkyYjk2MjBjNWEzZjJjYjgwNTViYzYyNTk0MmI2Y2QzMDM5LzZiNjhiYzVjYTJiZmQ4YTcxMTE5YWYwZTg0NTQ5Mjk2NjhjY2RhNmEzMzQ5NTVjY2M5NWQxMTRmYzhkMDgyZmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=FWXqsrGMzgHbB40Kkhj9aYell%7EYq7kqIWp9g0%7EHcbhShL0xvnF8HOcQ1SKy5WsUrgSt5Z07XsiwyD0L8JE1DEnunvAXA8IlMpiYMNaH%7Epq0bEQBOZuTWKL7mIgSDhbcszekCkZP55aGJAptq4BTxtglz9wmcTYco9mUkUk4Oo45MSqq8zBw8td5WhD%7E68sHIacf7iei0E4BZ5XiS8oJVYdF3wQVvE7p3hqnsDL%7EfzCnkPGNfefVuhfbccQRAB3mmkQHueL2U7PMr9eA15iaZVXsu1tA%7E-PLOW1ORGs-wlFmhu3fvai4LmKukyui22iqWVAjqLmtocwrcifq1Eo6LpQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.165.171.53, 18.165.171.9, 18.165.171.94, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.165.171.53|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 228941895 (218M) [application/json]\n",
      "Saving to: ‘llava_instruct_150k.json.1’\n",
      "\n",
      "llava_instruct_150k 100%[===================>] 218.34M   110MB/s    in 2.0s    \n",
      "\n",
      "2024-01-25 13:27:45 (110 MB/s) - ‘llava_instruct_150k.json.1’ saved [228941895/228941895]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5zCY4DO95Zm",
    "outputId": "34b312e4-83e0-450a-d80b-0720123dafa8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '000000033471',\n",
       " 'image': '000000033471.jpg',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nWhat are the colors of the bus in the image?'},\n",
       "  {'from': 'gpt', 'value': 'The bus in the image is white and red.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'What feature can be seen on the back of the bus?'},\n",
       "  {'from': 'gpt', 'value': 'The back of the bus features an advertisement.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Is the bus driving down the street or pulled off to the side?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The bus is driving down the street, which is crowded with people and other vehicles.'}]}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening JSON file - instruct150k\n",
    "f = open('llava_instruct_150k.json')\n",
    "\n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2 - ASSEMBLE TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWoVMp2C-D8b",
    "outputId": "2f04aeb0-8fa4-4361-9a9e-3638f1c8f899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 processed\n",
      "10000 processed\n",
      "20000 processed\n",
      "30000 processed\n",
      "40000 processed\n",
      "50000 processed\n",
      "60000 processed\n",
      "70000 processed\n",
      "80000 processed\n",
      "90000 processed\n",
      "100000 processed\n",
      "110000 processed\n",
      "120000 processed\n",
      "130000 processed\n",
      "140000 processed\n",
      "150000 processed\n"
     ]
    }
   ],
   "source": [
    "# create input pickle file by flattening the data\n",
    "data_instruct150_flatten = []\n",
    "r = 0\n",
    "\n",
    "for a_idx,d in enumerate(data):\n",
    "    image = d['image']\n",
    "    image_url = 'http://images.cocodataset.org/train2017/' + image\n",
    "    conv_iter = iter( d['conversations'])\n",
    "    for i in conv_iter:\n",
    "      gpt_ans = next(conv_iter)\n",
    "      if len(gpt_ans['value']) > 200: # filter out too long answers\n",
    "          continue\n",
    "      if i['from'] == 'human' and gpt_ans['from'] == 'gpt':\n",
    "        data_instruct150_flatten.append((image_url, i['value'].replace('<image>\\n','').replace('<image>',''),gpt_ans['value']))\n",
    "\n",
    "    if a_idx % 10000 == 0:\n",
    "      print(f\"{10000 * r} processed\")\n",
    "      r += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('http://images.cocodataset.org/train2017/000000033471.jpg',\n",
       " 'What are the colors of the bus in the image?',\n",
       " 'The bus in the image is white and red.')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_instruct150_flatten[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk after 100000 rows\n",
      "Writing to disk after 200000 rows\n",
      "Writing to disk after 300000 rows\n",
      "Writing to disk after 400000 rows\n",
      "Writing to disk after 500000 rows\n",
      "Writing to disk after 600000 rows\n",
      "Writing to disk after 700000 rows\n",
      "Writing to disk after 800000 rows\n",
      "Writing to disk after 900000 rows\n",
      "Writing to disk after 1000000 rows\n",
      "Writing to disk after 1100000 rows\n",
      "Writing to disk after 1200000 rows\n",
      "Writing to disk after 1300000 rows\n",
      "Writing to disk after 1400000 rows\n",
      "Writing to disk after 1500000 rows\n",
      "Writing to disk after 1600000 rows\n",
      "Writing to disk after 1700000 rows\n",
      "Writing to disk after 1800000 rows\n",
      "Writing to disk after 1900000 rows\n",
      "Writing to disk after 2000000 rows\n",
      "Writing to disk after 2100000 rows\n",
      "Writing to disk after 2200000 rows\n",
      "Writing to disk after 2300000 rows\n",
      "Writing to disk after 2400000 rows\n",
      "Writing to disk after 2500000 rows\n",
      "Writing to disk after 2600000 rows\n",
      "Writing to disk after 2700000 rows\n",
      "Writing to disk after 2800000 rows\n",
      "Writing to disk after 2900000 rows\n",
      "Writing to disk after 3000000 rows\n",
      "Writing to disk after 3100000 rows\n",
      "Writing to disk after 3200000 rows\n",
      "Writing to disk after 3300000 rows\n",
      "Writing to disk after 3400000 rows\n",
      "Writing to disk after 3500000 rows\n",
      "Writing to disk after 3600000 rows\n",
      "Writing to disk after 3700000 rows\n",
      "Writing to disk after 3800000 rows\n",
      "Writing to disk after 3900000 rows\n",
      "Writing to disk after 4000000 rows\n",
      "Writing to disk after 4100000 rows\n",
      "Writing to disk after 4200000 rows\n",
      "Writing to disk after 4300000 rows\n",
      "Writing to disk after 4400000 rows\n",
      "Writing to disk after 4500000 rows\n",
      "Writing to disk after 4600000 rows\n",
      "Writing to disk after 4700000 rows\n",
      "Writing to disk after 4800000 rows\n",
      "Writing to disk after 4900000 rows\n",
      "Writing to disk after 5000000 rows\n",
      "Writing to disk after 5100000 rows\n",
      "Writing to disk after 5200000 rows\n",
      "Writing to disk after 5300000 rows\n",
      "Writing to disk after 5400000 rows\n",
      "Writing to disk after 5500000 rows\n",
      "Writing to disk after 5600000 rows\n",
      "Writing to disk after 5700000 rows\n",
      "Writing to disk after 5800000 rows\n",
      "Writing to disk after 5900000 rows\n",
      "Writing to disk after 6000000 rows\n",
      "Writing to disk after 6100000 rows\n",
      "Writing to disk after 6200000 rows\n",
      "Writing to disk after 6300000 rows\n",
      "Writing to disk after 6400000 rows\n",
      "Writing to disk after 6500000 rows\n",
      "Writing to disk after 6600000 rows\n",
      "Writing to disk after 6700000 rows\n",
      "Writing to disk after 6800000 rows\n",
      "Writing to disk after 6900000 rows\n",
      "Writing to disk after 7000000 rows\n",
      "Writing to disk after 7100000 rows\n",
      "Writing to disk after 7200000 rows\n",
      "Writing to disk after 7300000 rows\n",
      "Writing to disk after 7400000 rows\n",
      "Writing to disk after 7500000 rows\n",
      "Writing to disk after 7600000 rows\n",
      "Writing to disk after 7700000 rows\n",
      "Writing to disk after 7800000 rows\n",
      "Writing to disk after 7900000 rows\n",
      "Writing to disk after 8000000 rows\n",
      "Writing to disk after 8100000 rows\n",
      "Writing to disk after 8200000 rows\n",
      "Writing to disk after 8300000 rows\n",
      "Writing to disk after 8400000 rows\n",
      "Writing to disk after 8500000 rows\n",
      "Writing to disk after 8600000 rows\n",
      "Writing to disk after 8700000 rows\n",
      "Writing to disk after 8800000 rows\n",
      "Writing to disk after 8900000 rows\n",
      "Writing to disk after 9000000 rows\n",
      "Writing to disk after 9100000 rows\n",
      "Writing to disk after 9200000 rows\n",
      "Writing to disk after 9300000 rows\n",
      "Writing to disk after 9400000 rows\n",
      "Writing to disk after 9500000 rows\n",
      "Writing to disk after 9600000 rows\n",
      "Writing to disk after 9700000 rows\n",
      "Writing to disk after 9800000 rows\n",
      "Writing to disk after 9900000 rows\n",
      "Writing to disk after 10000000 rows\n",
      "Writing to disk after 10100000 rows\n",
      "Writing to disk after 10200000 rows\n",
      "Writing to disk after 10300000 rows\n",
      "Writing to disk after 10400000 rows\n",
      "Writing to disk after 10500000 rows\n",
      "Writing to disk after 10600000 rows\n",
      "Writing to disk after 10700000 rows\n",
      "Writing to disk after 10800000 rows\n",
      "Writing to disk after 10900000 rows\n",
      "Writing to disk after 11000000 rows\n",
      "Writing to disk after 11100000 rows\n",
      "Writing to disk after 11200000 rows\n",
      "Writing to disk after 11300000 rows\n",
      "Writing to disk after 11400000 rows\n",
      "Writing to disk after 11500000 rows\n",
      "Writing to disk after 11600000 rows\n",
      "Writing to disk after 11700000 rows\n",
      "Writing to disk after 11800000 rows\n",
      "Writing to disk after 11900000 rows\n",
      "Writing to disk after 12000000 rows\n",
      "Writing to disk after 12100000 rows\n",
      "Writing to disk after 12200000 rows\n",
      "Writing to disk after 12300000 rows\n",
      "Writing to disk after 12400000 rows\n",
      "Writing to disk after 12500000 rows\n",
      "Writing to disk after 12600000 rows\n",
      "Writing to disk after 12700000 rows\n",
      "Writing to disk after 12800000 rows\n",
      "Writing to disk after 12900000 rows\n",
      "Writing to disk after 13000000 rows\n",
      "Writing to disk after 13100000 rows\n",
      "Writing to disk after 13200000 rows\n",
      "Writing to disk after 13300000 rows\n",
      "Writing to disk after 13400000 rows\n",
      "Writing to disk after 13500000 rows\n",
      "Writing to disk after 13600000 rows\n",
      "Writing to disk after 13700000 rows\n",
      "Writing to disk after 13800000 rows\n",
      "Writing to disk after 13900000 rows\n",
      "Writing to disk after 14000000 rows\n",
      "Writing to disk after 14100000 rows\n",
      "Writing to disk after 14200000 rows\n",
      "Writing to disk after 14300000 rows\n",
      "Writing to disk after 14400000 rows\n",
      "Writing to disk after 14500000 rows\n",
      "Writing to disk after 14600000 rows\n",
      "Writing to disk after 14700000 rows\n",
      "Writing to disk after 14800000 rows\n",
      "Writing to disk after 14900000 rows\n",
      "Writing to disk after 15000000 rows\n",
      "Writing to disk after 15100000 rows\n",
      "Writing to disk after 15200000 rows\n"
     ]
    }
   ],
   "source": [
    "# gpt like training dataset\n",
    "with open('train_token.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([['img_url','input','label']])\n",
    "    \n",
    "train_data_temp = []\n",
    "r = 1\n",
    "for df in data_instruct150_flatten[0:10]:\n",
    "  image_url = df[0]\n",
    "  image_q   = df[1]\n",
    "  image_a   = df[2]\n",
    "\n",
    "  # tokenise \n",
    "  ques_token = tokenizer(image_q, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
    "  ans_token  = tokenizer(image_a, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
    "\n",
    "  context_length = len(ques_token)\n",
    "  combo_q_a = torch.cat( [ques_token,ans_token])\n",
    "    \n",
    "  for al in range(len(ans_token)):   \n",
    "    input = combo_q_a[al : al + context_length].numpy()\n",
    "    label = combo_q_a[al + 1 : al + context_length + 1].numpy()\n",
    "    train_data_temp.append([image_url,input,label])\n",
    "    if len(train_data_temp) >= 1: # write to the file\n",
    "       print(f\"Writing to disk after {r * 100000} rows\")\n",
    "       r += 1\n",
    "       with open('train_token.csv', 'a', newline='') as file:\n",
    "          writer = csv.writer(file)\n",
    "          writer.writerows(train_data_temp)\n",
    "       train_data_temp = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOP-GNDK40Ff"
   },
   "source": [
    "# 3 - PYTORCH DATASET AND DATALOADER TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "dMQZedDH4vVa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_url</th>\n",
       "      <th>input</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[2061  389  262 7577  286  262 1323  287  262 ...</td>\n",
       "      <td>[ 389  262 7577  286  262 1323  287  262 2939 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[ 389  262 7577  286  262 1323  287  262 2939 ...</td>\n",
       "      <td>[ 262 7577  286  262 1323  287  262 2939   30 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[ 262 7577  286  262 1323  287  262 2939   30 ...</td>\n",
       "      <td>[7577  286  262 1323  287  262 2939   30  464 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[7577  286  262 1323  287  262 2939   30  464 ...</td>\n",
       "      <td>[ 286  262 1323  287  262 2939   30  464 1323 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[ 286  262 1323  287  262 2939   30  464 1323 ...</td>\n",
       "      <td>[ 262 1323  287  262 2939   30  464 1323  287 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[ 262 1323  287  262 2939   30  464 1323  287 ...</td>\n",
       "      <td>[1323  287  262 2939   30  464 1323  287  262 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[1323  287  262 2939   30  464 1323  287  262 ...</td>\n",
       "      <td>[ 287  262 2939   30  464 1323  287  262 2939 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[ 287  262 2939   30  464 1323  287  262 2939 ...</td>\n",
       "      <td>[ 262 2939   30  464 1323  287  262 2939  318 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[ 262 2939   30  464 1323  287  262 2939  318 ...</td>\n",
       "      <td>[2939   30  464 1323  287  262 2939  318 2330 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[2939   30  464 1323  287  262 2939  318 2330 ...</td>\n",
       "      <td>[  30  464 1323  287  262 2939  318 2330  290 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             img_url  \\\n",
       "0  http://images.cocodataset.org/train2017/000000...   \n",
       "1  http://images.cocodataset.org/train2017/000000...   \n",
       "2  http://images.cocodataset.org/train2017/000000...   \n",
       "3  http://images.cocodataset.org/train2017/000000...   \n",
       "4  http://images.cocodataset.org/train2017/000000...   \n",
       "5  http://images.cocodataset.org/train2017/000000...   \n",
       "6  http://images.cocodataset.org/train2017/000000...   \n",
       "7  http://images.cocodataset.org/train2017/000000...   \n",
       "8  http://images.cocodataset.org/train2017/000000...   \n",
       "9  http://images.cocodataset.org/train2017/000000...   \n",
       "\n",
       "                                               input  \\\n",
       "0  [2061  389  262 7577  286  262 1323  287  262 ...   \n",
       "1  [ 389  262 7577  286  262 1323  287  262 2939 ...   \n",
       "2  [ 262 7577  286  262 1323  287  262 2939   30 ...   \n",
       "3  [7577  286  262 1323  287  262 2939   30  464 ...   \n",
       "4  [ 286  262 1323  287  262 2939   30  464 1323 ...   \n",
       "5  [ 262 1323  287  262 2939   30  464 1323  287 ...   \n",
       "6  [1323  287  262 2939   30  464 1323  287  262 ...   \n",
       "7  [ 287  262 2939   30  464 1323  287  262 2939 ...   \n",
       "8  [ 262 2939   30  464 1323  287  262 2939  318 ...   \n",
       "9  [2939   30  464 1323  287  262 2939  318 2330 ...   \n",
       "\n",
       "                                               label  \n",
       "0  [ 389  262 7577  286  262 1323  287  262 2939 ...  \n",
       "1  [ 262 7577  286  262 1323  287  262 2939   30 ...  \n",
       "2  [7577  286  262 1323  287  262 2939   30  464 ...  \n",
       "3  [ 286  262 1323  287  262 2939   30  464 1323 ...  \n",
       "4  [ 262 1323  287  262 2939   30  464 1323  287 ...  \n",
       "5  [1323  287  262 2939   30  464 1323  287  262 ...  \n",
       "6  [ 287  262 2939   30  464 1323  287  262 2939 ...  \n",
       "7  [ 262 2939   30  464 1323  287  262 2939  318 ...  \n",
       "8  [2939   30  464 1323  287  262 2939  318 2330 ...  \n",
       "9  [  30  464 1323  287  262 2939  318 2330  290 ...  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv('train_token.csv')\n",
    "df_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "id": "iSQeuVRR4_W7"
   },
   "outputs": [],
   "source": [
    "class llavadataset(Dataset):\n",
    "  def __init__(self, qa_dataset, phi_model_name, clip_model_name, tokenizer):\n",
    "    self.processor  = AutoProcessor.from_pretrained(clip_model_name)\n",
    "    self.qa_dataset = qa_dataset\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.qa_dataset.shape[0]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # from image perspective\n",
    "    img_url = self.qa_dataset.img_url[idx]\n",
    "    ques    = torch.tensor(np.array(np.matrix(self.qa_dataset.input[idx]))[0])  \n",
    "    ans     = torch.tensor(np.array(np.matrix(self.qa_dataset.label[idx]))[0])\n",
    "    \n",
    "    # image load\n",
    "    image_load = Image.open(requests.get(img_url,stream=True).raw)\n",
    "    image_processed = self.processor(images=image_load, return_tensors=\"pt\") ['pixel_values']\n",
    "    image_processed = image_processed.squeeze(0)\n",
    "    # q = self.tokenizer(ques, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
    "    # a = self.tokenizer(ans, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
    "    return(image_processed , ques, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Phk2K52q5wy1",
    "outputId": "4ed8cebb-2b0a-4b0c-a32b-11ae53031835"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "phi_model_name  = \"microsoft/phi-2\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
    "csv_file = 'train_token.csv'\n",
    "qa_dataset = pd.read_csv(csv_file)\n",
    "# train_dataset,test_dataset = train_test_split(qa_dataset, test_size=0.1)\n",
    "\n",
    "# train_dataset.reset_index(inplace=True)\n",
    "# test_dataset.reset_index(inplace=True)\n",
    "step2_dataset = llavadataset(qa_dataset, phi_model_name, clip_model_name, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JarVPVPe6IwR",
    "outputId": "ffed174a-8ce7-4d1a-ac1a-7afba1df9316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 389,  262, 7577,  286,  262, 1323,  287,  262, 2939,   30,  464])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.1858,  1.1566,  1.1274,  ...,  1.3902,  1.3756,  1.1712],\n",
       "          [ 1.2588,  1.2296,  1.1858,  ...,  1.4048,  1.2150,  1.2734],\n",
       "          [ 1.4194,  1.3318,  1.2880,  ...,  1.3610,  1.1128,  1.3756],\n",
       "          ...,\n",
       "          [-0.4346, -0.3762, -0.3762,  ...,  0.8209,  0.7041,  0.9522],\n",
       "          [-0.4200, -0.4054, -0.3908,  ...,  0.1493,  0.1639,  0.1055],\n",
       "          [-0.4346, -0.4346, -0.3908,  ...,  0.2223,  0.1493,  0.1347]],\n",
       " \n",
       "         [[ 1.4145,  1.3695,  1.3395,  ...,  1.6397,  1.6247,  1.3995],\n",
       "          [ 1.4596,  1.4446,  1.3995,  ...,  1.6397,  1.4446,  1.5046],\n",
       "          [ 1.5646,  1.4896,  1.4295,  ...,  1.5946,  1.3395,  1.6096],\n",
       "          ...,\n",
       "          [-0.4014, -0.3264, -0.3264,  ...,  0.9793,  0.7542,  1.0393],\n",
       "          [-0.3564, -0.3414, -0.3264,  ...,  0.2439,  0.2289,  0.1839],\n",
       "          [-0.3714, -0.3714, -0.3264,  ...,  0.2439,  0.1989,  0.1839]],\n",
       " \n",
       "         [[ 1.7904,  1.7904,  1.7477,  ...,  1.9895,  1.9610,  1.7477],\n",
       "          [ 1.8473,  1.8331,  1.7904,  ...,  1.9895,  1.8046,  1.8473],\n",
       "          [ 1.9326,  1.8899,  1.8331,  ...,  1.9468,  1.7051,  1.9610],\n",
       "          ...,\n",
       "          [-0.1009, -0.0440, -0.0440,  ...,  1.1647,  0.8661,  1.1932],\n",
       "          [-0.0867, -0.0582, -0.0440,  ...,  0.3826,  0.4395,  0.3399],\n",
       "          [-0.1151, -0.1009, -0.0440,  ...,  0.4679,  0.4110,  0.3826]]]),\n",
       " tensor([ 389,  262, 7577,  286,  262, 1323,  287,  262, 2939,   30,  464]),\n",
       " tensor([ 262, 7577,  286,  262, 1323,  287,  262, 2939,   30,  464, 1323]))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "PG93q8ZR_rlg"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    image_embeddings, ques, ans = zip(*batch)\n",
    "    image_embeddings_stacked = torch.stack(image_embeddings, dim=0)\n",
    "    ques_padded = torch.nn.utils.rnn.pad_sequence(ques, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    ans_padded = torch.nn.utils.rnn.pad_sequence(ans, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return (image_embeddings_stacked, ques_padded,ans_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "id": "qTKSKcD0Au-3"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbZ6KsB3BjMR",
    "outputId": "856a5ef9-9719-4499-c76b-12f5c52c233b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ds3UxzU_eTA",
    "outputId": "0f5d1815-68a3-41a0-8225-8d14a47473fb"
   },
   "outputs": [],
   "source": [
    "val_dataloader   = DataLoader(llavadataset(qa_dataset, phi_model_name,clip_model_name,tokenizer),\n",
    "                      collate_fn=collate_fn, batch_size=2, num_workers = 10, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txeV8H8c_267",
    "outputId": "8b5ec949-a056-431b-ac36-699c3f8129d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  464,  3797,   318, 19378,   319,  1353,   286,   262,   736,   286,\n",
      "          262, 18507,   287])tensor([  661,   287,   262,  2939, 41348, 35390,   393,  3272,    12, 19315,\n",
      "        41348,    30,   464,   661])tensor([  464,   661,   287,   262,  2939,   389,  3272,    12, 19315, 41348,\n",
      "          287,   262, 16479,    11,   355,   484,   389, 41348,   319,   257])tensor([  464,  3797,   318,  2406,   503,   422,   617, 41160,  4291,   262,\n",
      "        18507,   290,   318,  5586,   393,  5055,   319])tensor([  262,  2939, 41348, 35390,   393,  3272,    12, 19315, 41348,    30,\n",
      "          464,   661,   287,   262])tensor([ 866,  262, 4675,  393, 5954,  572,  284,  262, 1735,   30,  464, 1323,\n",
      "         318, 5059])tensor([ 2061,  1611,   286,  2119,   857,   262, 18507,  1656,   284,   307,\n",
      "          287,    30])tensor([2061,  389,  262, 7577,  286,  262, 1323,  287,  262, 2939,   30])tensor([  262,  3797,    30,   464,  3124,   286,   262, 41160,  1474,   262,\n",
      "         3797])tensor([ 318,  262, 3797, 1804,  287,  262, 2939,   30,  464])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor([  464,   661,   287,   262,  2939,   389,  3272,    12, 19315, 41348,\n",
      "          287,   262, 16479,    11,   355,   484,   389, 41348,   319,   257,\n",
      "         8025,  2138,   621,   257, 14559, 22638])tensor([1858,  389,  734,  661,  287,  262, 2939,   11, 1111])\n",
      "\n",
      "tensor([19315, 41348,    30,   464,   661,   287,   262,  2939,   389,  3272,\n",
      "           12, 19315, 41348,   287])\n",
      "tensor([  464,  3797,   318,  2406,   503,   422,   617, 41160,  4291,   262,\n",
      "        18507,   290,   318,  5586,   393,  5055,   319,  1353,   286])tensor([35390,   393,  3272,    12, 19315, 41348,    30,   464,   661,   287,\n",
      "          262,  2939,   389,  3272])\n",
      "\n",
      "tensor([ 1474,   262,  3797,    30,   464,  3124,   286,   262, 41160,  1474,\n",
      "          262])\n",
      "tensor([  464,   661,   287,   262,  2939,   389,  3272,    12, 19315, 41348,\n",
      "          287,   262, 16479,    11,   355,   484,   389, 41348])\n",
      "tensor([ 2061,   318,   262,  3124,   286,   262, 41160,  1474,   262,  3797,\n",
      "           30])\n",
      "tensor([ 2073,   460,   307,  1775,   319,   262, 18507,    30,  1858,   389,\n",
      "         6041,   286,  9582])\n",
      "tensor([  460,   307,  1775,   319,   262, 18507,    30,  1858,   389,  6041,\n",
      "          286,  9582,  1666])\n",
      "tensor([ 3797,    30,   464,  3124,   286,   262, 41160,  1474,   262,  3797,\n",
      "          318])\n",
      "tensor([18507,    30,  1858,   389,  6041,   286,  9582,  1666,   319,   262,\n",
      "        18507, 13769,   262])\n",
      "tensor([ 1611,   286,  2119,   857,   262, 18507,  1656,   284,   307,   287,\n",
      "           30,   464])\n",
      "tensor([  262,  2939,    30,   198,    27,  9060,    29,   464,  3797,   318,\n",
      "        19378,   319,  1353])\n",
      "tensor([  307,   287,    30,   464, 18507,  3568,   284,   307,   287,   257,\n",
      "         2877,  2119])\n",
      "tensor([41160,  1474,   262,  3797,    30,   464,  3124,   286,   262, 41160,\n",
      "         1474])\n",
      "tensor([  464,   661,   287,   262,  2939,   389,  3272,    12, 19315, 41348,\n",
      "          287,   262, 16479,    11,   355,   484,   389, 41348,   319,   257,\n",
      "         8025,  2138])\n",
      "tensor([  464,  3797,   318,  2406,   503,   422,   617, 41160,  4291,   262,\n",
      "        18507,   290,   318])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[[[-1.1791, -1.1207, -1.0331,  ..., -1.0915, -0.6390, -0.0405],\n",
       "           [-1.3835, -1.3397, -0.7704,  ..., -1.0331, -0.6098, -0.3908],\n",
       "           [-0.4930, -0.9310, -0.5806,  ..., -1.0623, -1.0477, -0.5660],\n",
       "           ...,\n",
       "           [ 1.1274,  1.1566,  1.1712,  ...,  0.8792,  0.6895,  0.6895],\n",
       "           [ 1.1566,  1.1566,  1.1566,  ...,  0.8209,  0.7917,  0.8792],\n",
       "           [ 1.1274,  1.1128,  1.1274,  ...,  0.7917,  0.8792,  0.8647]],\n",
       " \n",
       "          [[-0.9417, -0.8967, -0.8066,  ..., -0.8516, -0.2963,  0.1389],\n",
       "           [-1.1368, -1.1668, -0.6565,  ..., -0.8666, -0.2663, -0.2513],\n",
       "           [-0.2963, -0.8066, -0.3864,  ..., -0.9117, -0.8816, -0.4014],\n",
       "           ...,\n",
       "           [ 1.2945,  1.3245,  1.3545,  ...,  1.0694,  0.8743,  0.8893],\n",
       "           [ 1.3395,  1.3545,  1.3395,  ...,  0.9943,  0.9793,  1.0844],\n",
       "           [ 1.3395,  1.3095,  1.3245,  ...,  0.9343,  1.0844,  1.0844]],\n",
       " \n",
       "          [[-0.8545, -0.8403, -0.5844,  ..., -0.7123, -0.2146,  0.3542],\n",
       "           [-1.0536, -1.0110, -0.4137,  ..., -0.6981, -0.2289, -0.0156],\n",
       "           [-0.2431, -0.5986, -0.2715,  ..., -0.7692, -0.7123, -0.2289],\n",
       "           ...,\n",
       "           [ 1.4776,  1.4918,  1.5060,  ...,  1.2643,  1.0510,  1.0367],\n",
       "           [ 1.4918,  1.5060,  1.4918,  ...,  1.1789,  1.1505,  1.2358],\n",
       "           [ 1.4776,  1.4633,  1.4776,  ...,  1.1505,  1.2643,  1.2643]]],\n",
       " \n",
       " \n",
       "         [[[-0.9310, -1.0185, -0.8580,  ..., -0.1864, -0.2302, -0.2886],\n",
       "           [-1.0331, -0.9893, -0.8726,  ..., -0.1280, -0.2302, -0.2594],\n",
       "           [-1.1499, -1.0039, -0.8580,  ..., -0.1426, -0.2302, -0.2302],\n",
       "           ...,\n",
       "           [-0.7120, -0.6682, -0.6974,  ...,  1.3318,  1.2880,  1.3026],\n",
       "           [-0.6974, -0.6098, -0.6682,  ...,  1.7844,  1.7260,  1.6238],\n",
       "           [-0.5952, -0.6828, -0.7120,  ...,  1.9157,  1.9157,  1.9011]],\n",
       " \n",
       "          [[-0.8666, -1.4669, -1.4519,  ..., -1.2268, -1.3019, -1.3019],\n",
       "           [-1.2118, -1.4519, -1.4669,  ..., -1.2568, -1.2568, -1.2568],\n",
       "           [-1.3919, -1.5870, -1.2268,  ..., -1.2418, -1.2418, -1.2418],\n",
       "           ...,\n",
       "           [-0.5815, -0.6565, -0.6265,  ...,  1.8498,  1.8498,  1.7747],\n",
       "           [-0.6115, -0.7316, -0.5965,  ...,  2.0299,  2.0299,  1.9098],\n",
       "           [-0.6265, -0.7316, -0.6415,  ...,  2.0299,  2.0299,  2.0449]],\n",
       " \n",
       "          [[-0.8972, -1.3238, -1.2811,  ..., -1.0252, -0.9967, -1.0252],\n",
       "           [-1.1958, -1.3807, -1.1958,  ..., -0.9541, -1.0110, -1.0536],\n",
       "           [-1.3096, -1.3380, -1.0821,  ..., -0.9256, -1.0252, -0.9967],\n",
       "           ...,\n",
       "           [-0.5133, -0.5275, -0.4706,  ...,  2.0321,  2.0464,  2.0606],\n",
       "           [-0.5701, -0.6128, -0.5417,  ...,  2.1175,  2.1175,  2.0464],\n",
       "           [-0.5701, -0.7123, -0.5844,  ...,  2.1175,  2.1175,  2.1175]]]]),\n",
       " tensor([[  464,   661,   287,   262,  2939,   389,  3272,    12, 19315, 41348,\n",
       "            287,   262, 16479,    11,   355,   484,   389, 41348,   319,   257],\n",
       "         [  464,  3797,   318,  2406,   503,   422,   617, 41160,  4291,   262,\n",
       "          18507,   290,   318,  5586,   393,  5055,   319,  1353,   286, 50256]]),\n",
       " tensor([[  464,   661,   287,   262,  2939,   389,  3272,    12, 19315, 41348,\n",
       "            287,   262, 16479,    11,   355,   484,   389, 41348,   319,   257,\n",
       "           8025],\n",
       "         [  464,  3797,   318,  2406,   503,   422,   617, 41160,  4291,   262,\n",
       "          18507,   290,   318,  5586,   393,  5055,   319,  1353,   286,   340,\n",
       "          50256]])]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - SAMPLE VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json\n",
    "# Opening JSON file - instruct150k\n",
    "f = open('llava_instruct_150k.json')\n",
    "\n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input pickle file by flattening the data\n",
    "data_instruct150_sample_val_flatten = []\n",
    "r = 0\n",
    "\n",
    "for a_idx,d in enumerate(data):\n",
    "    image = d['image']\n",
    "    image_url = 'http://images.cocodataset.org/train2017/' + image\n",
    "    conv_iter = iter( d['conversations'])\n",
    "    for i in conv_iter:\n",
    "      gpt_ans = next(conv_iter)\n",
    "      if len(gpt_ans['value']) > 200: # filter out too long answers\n",
    "          continue\n",
    "      if i['from'] == 'human' and gpt_ans['from'] == 'gpt':\n",
    "        data_instruct150_sample_val_flatten.append([image_url, i['value'].replace('<image>\\n',''),gpt_ans['value']])\n",
    "\n",
    "    if a_idx % 10000 == 0:\n",
    "      print(f\"{10000 * r} processed\")\n",
    "      r += 1\n",
    "      if r >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_val_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([['img_url','q','a']])\n",
    "\n",
    "with open('sample_val_data.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data_instruct150_sample_val_flatten)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
