{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb04d7dd-1ed2-4d30-a96a-6dc35849eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import peft\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, CLIPVisionModel, AutoProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "from peft import PeftModel\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddb9d4a-245f-4262-823f-2124b9379ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "phi_model_name  = \"microsoft/phi-2\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
    "processor  = AutoProcessor.from_pretrained(clip_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "IMAGE_TOKEN_ID = 23893 # token for word comment\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_embed = 768\n",
    "phi_embed  = 2560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bed4e706-ee93-413e-aa88-61083f62677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResBlock(nn.Module):\n",
    "    def __init__(self, phi_embed):\n",
    "        super().__init__()\n",
    "        self.pre_norm = nn.LayerNorm(phi_embed)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(phi_embed, phi_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(phi_embed, phi_embed)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.pre_norm(x)\n",
    "        return x + self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "113549ce-d120-479b-9230-cdf4ff3309e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f312a1a7a3943b7b3df66a37cdbd87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# models\n",
    "clip_model = CLIPVisionModel.from_pretrained(clip_model_name).to(device)\n",
    "projection = torch.nn.Linear(clip_embed, phi_embed).to(device)\n",
    "resblock = SimpleResBlock(phi_embed).to(device)\n",
    "\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    phi_model_name,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3da7419e-8b2b-4923-aa00-94857c00e9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weights\n",
    "model_to_merge = PeftModel.from_pretrained(phi_model,'./model_chkpt/lora_adaptor')\n",
    "merged_model = model_to_merge.merge_and_unload().to(device)\n",
    "projection.load_state_dict(torch.load('./model_chkpt/step2_projection.pth',map_location=torch.device(device)))\n",
    "resblock.load_state_dict(torch.load('./model_chkpt/step2_resblock.pth',map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9380865-f7a2-446f-8bce-7f5303d40411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4252002-b774-403e-850c-258a3fc9efd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://b4778b584662cccb7c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b4778b584662cccb7c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model_generate_ans(img,val_q):\n",
    "\n",
    "    max_generate_length = 20\n",
    "    \n",
    "    # image\n",
    "    image_processed = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "    clip_val_outputs = clip_model(**image_processed).last_hidden_state[:,1:,:]\n",
    "    val_image_embeds = projection(clip_val_outputs)\n",
    "    val_image_embeds = resblock(val_image_embeds).to(torch.float16)\n",
    "\n",
    "    img_token_tensor = torch.tensor(IMAGE_TOKEN_ID).to(device)\n",
    "    img_token_embeds = merged_model.model.embed_tokens(img_token_tensor).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    val_q_tokenised = tokenizer(val_q, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0).to(device)\n",
    "    val_q_embeds    = merged_model.model.embed_tokens(val_q_tokenised).unsqueeze(0)\n",
    "    \n",
    "    val_combined_embeds = torch.cat([val_image_embeds, img_token_embeds, val_q_embeds], dim=1) # 4, 69, 2560\n",
    "    \n",
    "    predicted_caption = torch.full((1,max_generate_length),50256)\n",
    "\n",
    "    for g in range(max_generate_length):\n",
    "        phi_output_logits = merged_model(inputs_embeds=val_combined_embeds)['logits'] # 4, 69, 51200\n",
    "        predicted_word_token_logits = phi_output_logits[:, -1, :].unsqueeze(1) # 4,1,51200\n",
    "        predicted_word_token = torch.argmax(predicted_word_token_logits, dim = -1) # 4,1\n",
    "        predicted_caption[:,g] = predicted_word_token.view(1,-1).to(device)\n",
    "        next_token_embeds = phi_model.model.embed_tokens(predicted_word_token) # 4,1,2560\n",
    "        val_combined_embeds   = torch.cat([val_combined_embeds, next_token_embeds], dim=1)\n",
    "        \n",
    "    predicted_captions_decoded = tokenizer.batch_decode(predicted_caption,ignore_index = 50256)\n",
    "    \n",
    "    return predicted_captions_decoded\n",
    "    \n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Chat with MultiModal GPT !\n",
    "    Build using combining clip model and phi-2 model.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # app GUI\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            img_input    = gr.Image(label='Image')\n",
    "            img_question = gr.Text(label ='Question')\n",
    "        with gr.Column():\n",
    "            img_answer   = gr.Text(label ='Answer')\n",
    "\n",
    "    section_btn = gr.Button(\"Submit\")\n",
    "    section_btn.click(model_generate_ans, inputs=[img_input,img_question], outputs=[img_answer])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b638c6-f2a2-49ca-bcd9-4fe6dbdddc34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
