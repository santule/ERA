{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d11cb27-d931-4ac1-a1fc-3e79fa5c1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from step2_dataset import llavadataset, collate_fn\n",
    "import pickle\n",
    "import peft\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer,BitsAndBytesConfig, AutoModelForCausalLM, CLIPVisionModel, AutoProcessor\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import pandas as pd\n",
    "from torch.nn import functional as F\n",
    "import csv\n",
    "import random\n",
    "from PIL import Image\n",
    "import requests\n",
    "import wandb\n",
    "import os\n",
    "from peft import PeftModel\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1d568a-3fe5-4130-a504-9f8861920981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msanjanatule\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20240130_005324-k66m2maz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sanjanatule/clip_phi2_project/runs/k66m2maz' target=\"_blank\">step2_finetune</a></strong> to <a href='https://wandb.ai/sanjanatule/clip_phi2_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sanjanatule/clip_phi2_project' target=\"_blank\">https://wandb.ai/sanjanatule/clip_phi2_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sanjanatule/clip_phi2_project/runs/k66m2maz' target=\"_blank\">https://wandb.ai/sanjanatule/clip_phi2_project/runs/k66m2maz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(50296, 50256, {'input_ids': [50295], 'attention_mask': [1]})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "phi_model_name  = \"microsoft/phi-2\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
    "processor  = AutoProcessor.from_pretrained(clip_model_name)\n",
    "tokenizer.add_tokens('[QA]')\n",
    "tokenizer.add_special_tokens({'pad_token':'[PAD]'}) \n",
    "train_batch_size    = 32\n",
    "clip_embed = 768\n",
    "phi_embed  = 2560\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_workers = 10\n",
    "IMAGE_TOKEN_ID = 23893 # token for word comment\n",
    "max_steps      = 100000\n",
    "EOS_TOKEN_ID   = 50256\n",
    "phi_patches    = 49\n",
    "vocab_size     = 51200\n",
    "max_generate_length = 100\n",
    "model_val_step      = 1000\n",
    "model_log_step      = 100\n",
    "model_save_step     = 100\n",
    "wandb.init(project  = \"clip_phi2_project\", name=\"step2_finetune\")\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "tokenizer.pad_token_id, tokenizer.eos_token_id, tokenizer('[QA]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692e0ef-165c-40c5-a6a0-a601d1c5099c",
   "metadata": {},
   "source": [
    "# 1 - DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5844cdb1-2a30-4d65-a129-606d54e2b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "csv_file = 'train_token.csv'\n",
    "qa_dataset = pd.read_csv(csv_file)\n",
    "\n",
    "# data loaders\n",
    "train_dataloader = DataLoader(llavadataset(qa_dataset, phi_model_name,clip_model_name,processor),\n",
    "                  collate_fn=collate_fn, batch_size=train_batch_size, num_workers = num_workers, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bccc4c4-55d7-46db-aba1-5871bf88c840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://images.cocodataset.org/train2017/000000033471.jpg', 'What are the colors of the bus in the image? [QA]', 'The bus in the image is white and red.<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "file = open('sample_val_data.csv')\n",
    "csvreader = csv.reader(file)\n",
    "sample_val_data = []\n",
    "for row in csvreader:\n",
    "    sample_val_data.append(row)\n",
    "print(sample_val_data[1])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f57a66-8827-47b9-b006-d409d6ce4cc2",
   "metadata": {},
   "source": [
    "# 2 - MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a977832a-7109-4125-9570-388496c0e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResBlock(nn.Module):\n",
    "    def __init__(self, phi_embed):\n",
    "        super().__init__()\n",
    "        self.pre_norm = nn.LayerNorm(phi_embed)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(phi_embed, phi_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(phi_embed, phi_embed)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.pre_norm(x)\n",
    "        return x + self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef46a021-3811-4297-9604-b4d4b181cc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b46b357b1db4ace86975ef053b9f719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clip_model = CLIPVisionModel.from_pretrained(clip_model_name).to(device)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,)\n",
    "\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    phi_model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "phi_model.config.use_cache = False\n",
    "projection = torch.nn.Linear(clip_embed, phi_embed).to(device)\n",
    "resblock = SimpleResBlock(phi_embed).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e720ab-4f63-487d-ad9b-97ab0dd2300b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 83,886,080 || all params: 2,863,569,920 || trainable%: 2.9294231446599355\n"
     ]
    }
   ],
   "source": [
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'fc1',\n",
    "        'fc2'\n",
    "    ]\n",
    ")\n",
    "peft_model = peft.get_peft_model(phi_model, peft_config).to(device)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e219df78-1e2b-4730-81d6-5a7f794d8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip non trainable\n",
    "for network in [clip_model]:\n",
    "    for param in network.parameters():\n",
    "        param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9885fcad-2060-42c0-b2e9-e6b9469fde7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL:83886080\n",
      "PROJECTION MODEL:1968640\n",
      "CLIP MODEL:0\n",
      "PHI MODEL:83886080\n",
      "RESNET MODEL:13117440\n"
     ]
    }
   ],
   "source": [
    "# check trainable paramaeters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"PEFT MODEL:{count_parameters(peft_model)}\")\n",
    "print(f\"PROJECTION MODEL:{count_parameters(projection)}\")\n",
    "print(f\"CLIP MODEL:{count_parameters(clip_model)}\")\n",
    "print(f\"PHI MODEL:{count_parameters(phi_model)}\")\n",
    "print(f\"RESNET MODEL:{count_parameters(resblock)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad7fd86-4d63-41f6-a23e-81501fa08ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded step1 checkpoint\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('model_chkpt/step2_projection.pth'):\n",
    "    projection.load_state_dict(torch.load('./model_chkpt/step2_projection.pth'))\n",
    "    resblock.load_state_dict(torch.load('./model_chkpt/step2_resblock.pth'))\n",
    "    peft_model.from_pretrained(phi_model,'./model_chkpt/lora_adaptor')\n",
    "    print(\"Loaded step2 checkpoint\")\n",
    "\n",
    "else:\n",
    "    projection.load_state_dict(torch.load('./model_chkpt/step1_projection.pth'))\n",
    "    resblock.load_state_dict(torch.load('./model_chkpt/step1_resblock.pth'))\n",
    "    print(\"Loaded step1 checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745aeb44-44fe-4579-8d84-29e33e8a660f",
   "metadata": {},
   "source": [
    "# 3 - FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f950cd90-fb80-4991-9b9d-5855bded45b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: http://images.cocodataset.org/train2017/000000026263.jpg\n",
      "Question: Is there any writing or message on the banana? If so, what does it say? [QA]\n",
      "Answer:   Yes, there is a message written on the banana. It says, \"Sometimes a bit green; Often slippery; But always good for you!\"<|endoftext|>\n",
      "Model Predicted Ans: \n",
      "The banana has a label that reads \"Banana: The Fruit of the Month\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# random validation prediction\n",
    "def model_run_val(sample_val_data,max_generate_length=10):\n",
    "\n",
    "    total_val_len = len(sample_val_data)\n",
    "    random_val_datapoint = random.randrange(1,total_val_len) # 0 is header\n",
    "    \n",
    "    val_image_url = sample_val_data[random_val_datapoint][0]\n",
    "    val_q = sample_val_data[random_val_datapoint][1]\n",
    "    val_a = sample_val_data[random_val_datapoint][2]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_load = Image.open(requests.get(val_image_url,stream=True).raw)\n",
    "        image_processed = processor(images=image_load, return_tensors=\"pt\").to(device)\n",
    "        clip_val_outputs = clip_model(**image_processed).last_hidden_state[:,1:,:]\n",
    "        val_image_embeds = projection(clip_val_outputs)\n",
    "        val_image_embeds = resblock(val_image_embeds).to(torch.float16)\n",
    "        \n",
    "        \n",
    "        img_token_tensor = torch.tensor(IMAGE_TOKEN_ID).to(device)\n",
    "        img_token_embeds = peft_model.model.model.embed_tokens(img_token_tensor).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        val_q_tokenised = tokenizer(val_q, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
    "        val_q_embeds  = peft_model.model.model.embed_tokens(val_q_tokenised).unsqueeze(0)\n",
    "        \n",
    "        val_combined_embeds = torch.cat([val_image_embeds, img_token_embeds, val_q_embeds], dim=1) # 1, 69, 2560\n",
    "\n",
    "        predicted_caption = peft_model.generate(inputs_embeds=val_combined_embeds,\n",
    "                                                  max_new_tokens=max_generate_length,\n",
    "                                                  return_dict_in_generate = True)\n",
    "    \n",
    "        predicted_captions_decoded = tokenizer.batch_decode(predicted_caption.sequences[:, 1:])[0] \n",
    "        predicted_captions_decoded = predicted_captions_decoded.replace(\"<|endoftext|>\", \"\")\n",
    "\n",
    "    print(f\"Image: {val_image_url}\")\n",
    "    print(f\"Question: {val_q}\")\n",
    "    print(f\"Answer:   {val_a}\")\n",
    "    print(f\"Model Predicted Ans: {predicted_captions_decoded}\")\n",
    "    \n",
    "model_run_val(sample_val_data,max_generate_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896295f-05f9-4541-bfdd-ea41de8cbb14",
   "metadata": {},
   "source": [
    "# 3 - TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "932d0019-0cb7-4a6a-bc6e-bf7033374bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_optimizer        = torch.optim.Adam(peft_model.parameters(), lr=1e-6)\n",
    "projection_optimizer = torch.optim.Adam(projection.parameters(), lr=1e-5)\n",
    "resnet_optimizer     = torch.optim.Adam(resblock.parameters(),   lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b32e97f3-ff51-4b5c-9bcf-ef8c9ec4c033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0/100000, Loss: 5.283018589019775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: http://images.cocodataset.org/train2017/000000107535.jpg\n",
      "Question: What is the woman doing on the street corner? [QA]\n",
      "Answer:   The woman is standing near a pole on the sidewalk at the corner of a street, looking at ads posted on the pole and pushing a walk signal button on the street corner to safely cross the road.<|endoftext|>\n",
      "Model Predicted Ans: a woman is looking at a street sign in the middle of the street.\n",
      "\n",
      "The woman in the image is looking at a street sign, which suggests that she is in a street with streetlights and traffic.\n",
      "\n",
      "Saving Checkpoint\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Caught ConnectionError in DataLoader worker process 9.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 461, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n    response.begin()\n  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n    version, status, reason = self._read_status()\n  File \"/opt/conda/lib/python3.10/http/client.py\", line 279, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n    return self._sock.recv_into(b)\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 798, in urlopen\n    retries = retries.increment(\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 461, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n    response.begin()\n  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n    version, status, reason = self._read_status()\n  File \"/opt/conda/lib/python3.10/http/client.py\", line 279, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n    return self._sock.recv_into(b)\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/step2_dataset.py\", line 30, in __getitem__\n    image_load = Image.open(requests.get(img_url,stream=True).raw)\n  File \"/opt/conda/lib/python3.10/site-packages/requests/api.py\", line 73, in get\n    return request(\"get\", url, params=params, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/requests/api.py\", line 59, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m resblock\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000000\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images,questions,answers) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# process input data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m questions\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m         questions  \u001b[38;5;241m=\u001b[39m questions\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mConnectionError\u001b[0m: Caught ConnectionError in DataLoader worker process 9.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 461, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n    response.begin()\n  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n    version, status, reason = self._read_status()\n  File \"/opt/conda/lib/python3.10/http/client.py\", line 279, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n    return self._sock.recv_into(b)\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 798, in urlopen\n    retries = retries.increment(\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 461, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n    response.begin()\n  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n    version, status, reason = self._read_status()\n  File \"/opt/conda/lib/python3.10/http/client.py\", line 279, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n    return self._sock.recv_into(b)\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/step2_dataset.py\", line 30, in __getitem__\n    image_load = Image.open(requests.get(img_url,stream=True).raw)\n  File \"/opt/conda/lib/python3.10/site-packages/requests/api.py\", line 73, in get\n    return request(\"get\", url, params=params, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/requests/api.py\", line 59, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "running_loss = 0.\n",
    "projection.train()\n",
    "peft_model.train()\n",
    "resblock.train()\n",
    "\n",
    "for epoch in range(1000000):\n",
    "    for batch_idx, (images,questions,answers) in enumerate(train_dataloader):\n",
    "\n",
    "        # process input data\n",
    "        batch_size = questions.size(0)\n",
    "        questions  = questions.to(device)\n",
    "        answers    = answers.to(device)\n",
    "\n",
    "        # clip\n",
    "        images = {'pixel_values': images.to(device)}\n",
    "        clip_outputs  = clip_model(**images)\n",
    "        images_embeds = clip_outputs.last_hidden_state[:,1:,:] # remove cls token\n",
    "        \n",
    "        # projection\n",
    "        image_embeds  = projection(images_embeds)\n",
    "        image_embeds  = resblock(image_embeds).to(torch.float16)\n",
    "\n",
    "        # embeds\n",
    "        #print(questions.shape,answers.shape)\n",
    "        img_token_tensor = torch.tensor(IMAGE_TOKEN_ID).repeat(batch_size, 1).to(device)\n",
    "        img_token_embeds = peft_model.model.model.embed_tokens(img_token_tensor)\n",
    "        questions_embed  = peft_model.model.model.embed_tokens(questions)\n",
    "\n",
    "        # forward pass\n",
    "        #print(\"***************\")\n",
    "        combined_embeds = torch.cat([image_embeds, img_token_embeds, questions_embed], dim=1) # 4, 69, 2560\n",
    "        #print(f\"combined_embeds shape{combined_embeds.shape}\")\n",
    "        phi_output_logits = peft_model(inputs_embeds=combined_embeds)['logits'] # 4, 69, 51200\n",
    "        #print(f\"phi_output_logits shape{phi_output_logits.shape}\")\n",
    "        #print(f\"answers shape {answers.shape}\")\n",
    "\n",
    "        # take out the image embeddings\n",
    "        phi_output_logits = phi_output_logits[:,images_embeds.shape[1] + 1 : ,:]\n",
    "        #print(f\"phi_output_logits after shape{phi_output_logits.shape}\")\n",
    "        phi_output_logits = phi_output_logits.reshape(-1,vocab_size)\n",
    "        #print(f\"phi_output_logits after shape{phi_output_logits.shape}\")\n",
    "        #print(f\"answers after shape {answers.contiguous().view(-1).shape}\")\n",
    "\n",
    "        phi_optimizer.zero_grad()\n",
    "        projection_optimizer.zero_grad()\n",
    "        resnet_optimizer.zero_grad()\n",
    "        \n",
    "        loss = F.cross_entropy(phi_output_logits, answers.contiguous().view(-1), ignore_index=50296,label_smoothing=0.1)\n",
    "\n",
    "        # loss backprop\n",
    "        loss.backward()\n",
    "        phi_optimizer.step()\n",
    "        projection_optimizer.step()\n",
    "        resnet_optimizer.step()\n",
    "        \n",
    "\n",
    "        if step % model_log_step == 0:\n",
    "            print(f\"Iteration {step}/{max_steps}, Loss: {loss.item()}\")\n",
    "\n",
    "        if step % model_val_step == 0:\n",
    "            projection.eval()\n",
    "            peft_model.eval()\n",
    "            resblock.eval()\n",
    "            model_run_val(sample_val_data,max_generate_length)\n",
    "            projection.train()\n",
    "            peft_model.train()\n",
    "            resblock.train()\n",
    "\n",
    "        if step % model_save_step == 0:\n",
    "            print(\"Saving Checkpoint\")\n",
    "            torch.save(projection.state_dict(),'./model_chkpt/step2_projection.pth')\n",
    "            torch.save(resblock.state_dict(),'./model_chkpt/step2_resblock.pth')\n",
    "            peft_model.save_pretrained('./model_chkpt/lora_adaptor/', save_adapter=True, save_config=True)\n",
    "            \n",
    "        if step >= max_steps:\n",
    "            print(\"Training finished.\")\n",
    "            break\n",
    "            \n",
    "        wandb.log({\"step\": step, \"train_loss\": loss.item()})\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d4b60-d49a-484a-8c87-cd7bf356cbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
