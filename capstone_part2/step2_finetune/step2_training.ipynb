{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d11cb27-d931-4ac1-a1fc-3e79fa5c1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from step2_dataset import llavadataset, collate_fn\n",
    "import pickle\n",
    "import peft\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer,BitsAndBytesConfig, AutoModelForCausalLM, CLIPVisionModel, AutoProcessor\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import pandas as pd\n",
    "from torch.nn import functional as F\n",
    "import csv\n",
    "import random\n",
    "from PIL import Image\n",
    "import requests\n",
    "import wandb\n",
    "import os\n",
    "from peft import PeftModel\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba1d568a-3fe5-4130-a504-9f8861920981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vomj3jfp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">step2_finetune</strong> at: <a href='https://wandb.ai/sanjanatule/clip_phi2_project/runs/vomj3jfp' target=\"_blank\">https://wandb.ai/sanjanatule/clip_phi2_project/runs/vomj3jfp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code></code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vomj3jfp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0076a7c8db7491ab481b550653a468b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114164100339016, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20240128_011946-awayl2cd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sanjanatule/clip_phi2_project/runs/awayl2cd' target=\"_blank\">step2_finetune</a></strong> to <a href='https://wandb.ai/sanjanatule/clip_phi2_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sanjanatule/clip_phi2_project' target=\"_blank\">https://wandb.ai/sanjanatule/clip_phi2_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sanjanatule/clip_phi2_project/runs/awayl2cd' target=\"_blank\">https://wandb.ai/sanjanatule/clip_phi2_project/runs/awayl2cd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "phi_model_name  = \"microsoft/phi-2\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
    "processor  = AutoProcessor.from_pretrained(clip_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_batch_size    = 32\n",
    "clip_embed = 768\n",
    "phi_embed  = 2560\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_workers = 10\n",
    "IMAGE_TOKEN_ID = 23893 # token for word comment\n",
    "max_steps      = 100000\n",
    "EOS_TOKEN_ID   = 50256\n",
    "phi_patches    = 49\n",
    "vocab_size     = 51200\n",
    "max_generate_length = 100\n",
    "model_val_step      = 1000\n",
    "model_log_step      = 100\n",
    "model_save_step     = 100\n",
    "wandb.init(project  = \"clip_phi2_project\", name=\"step2_finetune\")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692e0ef-165c-40c5-a6a0-a601d1c5099c",
   "metadata": {},
   "source": [
    "# 1 - DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5844cdb1-2a30-4d65-a129-606d54e2b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "csv_file = 'train_token.csv'\n",
    "qa_dataset = pd.read_csv(csv_file)\n",
    "\n",
    "# data loaders\n",
    "train_dataloader = DataLoader(llavadataset(qa_dataset, phi_model_name,clip_model_name,tokenizer,processor),\n",
    "                  collate_fn=collate_fn, batch_size=train_batch_size, num_workers = num_workers, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bccc4c4-55d7-46db-aba1-5871bf88c840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://images.cocodataset.org/train2017/000000033471.jpg', 'What are the colors of the bus in the image?', 'The bus in the image is white and red.']\n"
     ]
    }
   ],
   "source": [
    "file = open('sample_val_data.csv')\n",
    "csvreader = csv.reader(file)\n",
    "sample_val_data = []\n",
    "for row in csvreader:\n",
    "    sample_val_data.append(row)\n",
    "print(sample_val_data[1])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f57a66-8827-47b9-b006-d409d6ce4cc2",
   "metadata": {},
   "source": [
    "# 2 - MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a977832a-7109-4125-9570-388496c0e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResBlock(nn.Module):\n",
    "    def __init__(self, phi_embed):\n",
    "        super().__init__()\n",
    "        self.pre_norm = nn.LayerNorm(phi_embed)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(phi_embed, phi_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(phi_embed, phi_embed)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.pre_norm(x)\n",
    "        return x + self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef46a021-3811-4297-9604-b4d4b181cc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99621612d13549ca904f9d542011049d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clip_model = CLIPVisionModel.from_pretrained(clip_model_name).to(device)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,)\n",
    "\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    phi_model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "phi_model.config.use_cache = False\n",
    "projection = torch.nn.Linear(clip_embed, phi_embed).to(device)\n",
    "resblock = SimpleResBlock(phi_embed).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e720ab-4f63-487d-ad9b-97ab0dd2300b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 83,886,080 || all params: 2,863,569,920 || trainable%: 2.9294231446599355\n"
     ]
    }
   ],
   "source": [
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'fc1',\n",
    "        'fc2'\n",
    "    ]\n",
    ")\n",
    "peft_model = peft.get_peft_model(phi_model, peft_config).to(device)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e219df78-1e2b-4730-81d6-5a7f794d8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip non trainable\n",
    "for network in [clip_model]:\n",
    "    for param in network.parameters():\n",
    "        param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9885fcad-2060-42c0-b2e9-e6b9469fde7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL:83886080\n",
      "PROJECTION MODEL:1968640\n",
      "CLIP MODEL:0\n",
      "PHI MODEL:83886080\n",
      "RESNET MODEL:13117440\n"
     ]
    }
   ],
   "source": [
    "# check trainable paramaeters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"PEFT MODEL:{count_parameters(peft_model)}\")\n",
    "print(f\"PROJECTION MODEL:{count_parameters(projection)}\")\n",
    "print(f\"CLIP MODEL:{count_parameters(clip_model)}\")\n",
    "print(f\"PHI MODEL:{count_parameters(phi_model)}\")\n",
    "print(f\"RESNET MODEL:{count_parameters(resblock)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad7fd86-4d63-41f6-a23e-81501fa08ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded step2 checkpoint\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('model_chkpt/step2_projection.pth'):\n",
    "    projection.load_state_dict(torch.load('./model_chkpt/step2_projection.pth'))\n",
    "    resblock.load_state_dict(torch.load('./model_chkpt/step2_resblock.pth'))\n",
    "    peft_model.from_pretrained(phi_model,'./model_chkpt/lora_adaptor')\n",
    "    print(\"Loaded step2 checkpoint\")\n",
    "\n",
    "else:\n",
    "    projection.load_state_dict(torch.load('./model_chkpt/step1_projection.pth'))\n",
    "    resblock.load_state_dict(torch.load('./model_chkpt/step1_resblock.pth'))\n",
    "    print(\"Loaded step1 checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ce5193f-7251-41ed-b2b6-80b6ee613544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://images.cocodataset.org/train2017/000000033471.jpg',\n",
       " 'What are the colors of the bus in the image?',\n",
       " 'The bus in the image is white and red.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_val_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745aeb44-44fe-4579-8d84-29e33e8a660f",
   "metadata": {},
   "source": [
    "# 3 - FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f950cd90-fb80-4991-9b9d-5855bded45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random validation prediction\n",
    "def model_run_val(sample_val_data,max_generate_length=10):\n",
    "\n",
    "    total_val_len = len(sample_val_data)\n",
    "    random_val_datapoint = random.randrange(1,total_val_len) # 0 is header\n",
    "    \n",
    "    val_image_url = sample_val_data[random_val_datapoint][0]\n",
    "    val_q = sample_val_data[random_val_datapoint][1]\n",
    "    val_a = sample_val_data[random_val_datapoint][2]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_load = Image.open(requests.get(val_image_url,stream=True).raw)\n",
    "        image_processed = processor(images=image_load, return_tensors=\"pt\").to(device)\n",
    "        clip_val_outputs = clip_model(**image_processed).last_hidden_state[:,1:,:]\n",
    "        val_image_embeds = projection(clip_val_outputs)\n",
    "        val_image_embeds = resblock(val_image_embeds).to(torch.float16)\n",
    "        \n",
    "        \n",
    "        img_token_tensor = torch.tensor(IMAGE_TOKEN_ID).to(device)\n",
    "        img_token_embeds = peft_model.model.model.embed_tokens(img_token_tensor).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        val_q_tokenised = tokenizer(val_q, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
    "        val_q_embeds  = peft_model.model.model.embed_tokens(val_q_tokenised).unsqueeze(0)\n",
    "        \n",
    "        val_combined_embeds = torch.cat([val_image_embeds, img_token_embeds, val_q_embeds], dim=1) # 1, 69, 2560\n",
    "        \n",
    "        predicted_caption = torch.full((1,max_generate_length),50256)\n",
    "        \n",
    "        for g in range(max_generate_length):\n",
    "            phi_output_logits = peft_model(inputs_embeds=val_combined_embeds)['logits'] # 4, 69, 51200\n",
    "            predicted_word_token_logits = phi_output_logits[:, -1, :].unsqueeze(1) # 4,1,51200\n",
    "            predicted_word_token   = torch.argmax(predicted_word_token_logits, dim = -1) # 4,1\n",
    "            predicted_caption[:,g] = predicted_word_token.view(1,-1).to('cpu')\n",
    "            next_token_embeds      = phi_model.model.embed_tokens(predicted_word_token) # 4,1,2560\n",
    "            val_combined_embeds    = torch.cat([val_combined_embeds, next_token_embeds], dim=1)\n",
    "            \n",
    "        predicted_captions_decoded = tokenizer.batch_decode(predicted_caption,ignore_index = 50256)[0]\n",
    "\n",
    "    print(f\"Image: {val_image_url}\")\n",
    "    print(f\"Question: {val_q}\")\n",
    "    print(f\"Answer:   {val_a}\")\n",
    "    print(f\"Model Predicted Ans: {predicted_captions_decoded}\")\n",
    "    \n",
    "model_run_val(sample_val_data,max_generate_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896295f-05f9-4541-bfdd-ea41de8cbb14",
   "metadata": {},
   "source": [
    "# 3 - TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "932d0019-0cb7-4a6a-bc6e-bf7033374bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_optimizer        = torch.optim.Adam(peft_model.parameters(), lr=1e-6)\n",
    "projection_optimizer = torch.optim.Adam(projection.parameters(), lr=1e-5)\n",
    "resnet_optimizer     = torch.optim.Adam(resblock.parameters(),   lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b32e97f3-ff51-4b5c-9bcf-ef8c9ec4c033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0/100000, Loss: 1.6345438957214355\n",
      "Image: http://images.cocodataset.org/train2017/000000065554.jpg\n",
      "Question: What is the player wearing?\n",
      "Answer:   The player is wearing a black and white shirt.\n",
      "Model Predicted Ans: ['The player in the image is a tennis player in the tennis court, with the tennis racket in their']\n",
      "Saving Checkpoint\n",
      "Iteration 100/100000, Loss: 1.6121845245361328\n",
      "Saving Checkpoint\n",
      "Iteration 200/100000, Loss: 1.6203844547271729\n",
      "Saving Checkpoint\n",
      "Iteration 300/100000, Loss: 1.6238147020339966\n",
      "Saving Checkpoint\n",
      "Iteration 400/100000, Loss: 1.5800142288208008\n",
      "Saving Checkpoint\n",
      "Iteration 500/100000, Loss: 1.6031336784362793\n",
      "Saving Checkpoint\n",
      "Iteration 600/100000, Loss: 1.6001994609832764\n",
      "Saving Checkpoint\n",
      "Iteration 700/100000, Loss: 1.6003059148788452\n",
      "Saving Checkpoint\n",
      "Iteration 800/100000, Loss: 1.6222413778305054\n",
      "Saving Checkpoint\n",
      "Iteration 900/100000, Loss: 1.6048749685287476\n",
      "Saving Checkpoint\n",
      "Iteration 1000/100000, Loss: 1.6247951984405518\n",
      "Image: http://images.cocodataset.org/train2017/000000542054.jpg\n",
      "Question: What is the cat doing in the image?\n",
      "Answer:   The cat is standing on top of a bookshelf filled with books, hunching up and arching its back, likely stretching out. It is also looking into a cup, possibly a coffee mug, placed on the bookshelf.\n",
      "Model Predicted Ans: ['The cat is drinking from a coffee mug.The cat is drinking from the coffee and the cat is']\n",
      "Saving Checkpoint\n",
      "Iteration 1100/100000, Loss: 1.598085880279541\n",
      "Saving Checkpoint\n",
      "Iteration 1200/100000, Loss: 1.5743190050125122\n",
      "Saving Checkpoint\n",
      "Iteration 1300/100000, Loss: 1.5996801853179932\n",
      "Saving Checkpoint\n",
      "Iteration 1400/100000, Loss: 1.5906693935394287\n",
      "Saving Checkpoint\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m resblock\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000000\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images,questions,answers) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# process input data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m questions\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m         questions  \u001b[38;5;241m=\u001b[39m questions\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "running_loss = 0.\n",
    "projection.train()\n",
    "peft_model.train()\n",
    "resblock.train()\n",
    "\n",
    "for epoch in range(1000000):\n",
    "    for batch_idx, (images,questions,answers) in enumerate(train_dataloader):\n",
    "\n",
    "        # process input data\n",
    "        batch_size = questions.size(0)\n",
    "        questions  = questions.to(device)\n",
    "        answers    = answers.to(device)\n",
    "\n",
    "        # clip\n",
    "        images = {'pixel_values': images.to(device)}\n",
    "        clip_outputs  = clip_model(**images)\n",
    "        images_embeds = clip_outputs.last_hidden_state[:,1:,:] # remove cls token\n",
    "        \n",
    "        # projection\n",
    "        image_embeds  = projection(images_embeds)\n",
    "        image_embeds  = resblock(image_embeds).to(torch.float16)\n",
    "\n",
    "        # embeds\n",
    "        #print(questions.shape,answers.shape)\n",
    "        img_token_tensor = torch.tensor(IMAGE_TOKEN_ID).repeat(batch_size, 1).to(device)\n",
    "        img_token_embeds = peft_model.model.model.embed_tokens(img_token_tensor)\n",
    "        questions_embed  = peft_model.model.model.embed_tokens(questions)\n",
    "\n",
    "        # forward pass\n",
    "        #print(\"***************\")\n",
    "        combined_embeds = torch.cat([image_embeds, img_token_embeds, questions_embed], dim=1) # 4, 69, 2560\n",
    "        #print(f\"combined_embeds shape{combined_embeds.shape}\")\n",
    "        phi_output_logits = peft_model(inputs_embeds=combined_embeds)['logits'] # 4, 69, 51200\n",
    "        #print(f\"phi_output_logits shape{phi_output_logits.shape}\")\n",
    "        #print(f\"answers shape {answers.shape}\")\n",
    "\n",
    "        # take out the image embeddings\n",
    "        phi_output_logits = phi_output_logits[:,images_embeds.shape[1] + 1 : ,:]\n",
    "        #print(f\"phi_output_logits after shape{phi_output_logits.shape}\")\n",
    "        phi_output_logits = phi_output_logits.reshape(-1,vocab_size)\n",
    "        #print(f\"phi_output_logits after shape{phi_output_logits.shape}\")\n",
    "        #print(f\"answers after shape {answers.contiguous().view(-1).shape}\")\n",
    "\n",
    "        phi_optimizer.zero_grad()\n",
    "        projection_optimizer.zero_grad()\n",
    "        resnet_optimizer.zero_grad()\n",
    "        \n",
    "        loss = F.cross_entropy(phi_output_logits, answers.contiguous().view(-1), ignore_index=EOS_TOKEN_ID,label_smoothing=0.1)\n",
    "\n",
    "        # loss backprop\n",
    "        loss.backward()\n",
    "        phi_optimizer.step()\n",
    "        projection_optimizer.step()\n",
    "        resnet_optimizer.step()\n",
    "        \n",
    "\n",
    "        if step % model_log_step == 0:\n",
    "            print(f\"Iteration {step}/{max_steps}, Loss: {loss.item()}\")\n",
    "\n",
    "        if step % model_val_step == 0:\n",
    "            projection.eval()\n",
    "            peft_model.eval()\n",
    "            resblock.eval()\n",
    "            model_run_val(sample_val_data,max_generate_length)\n",
    "            projection.train()\n",
    "            peft_model.train()\n",
    "            resblock.train()\n",
    "\n",
    "        if step % model_save_step == 0:\n",
    "            print(\"Saving Checkpoint\")\n",
    "            torch.save(projection.state_dict(),'./model_chkpt/step2_projection.pth')\n",
    "            torch.save(resblock.state_dict(),'./model_chkpt/step2_resblock.pth')\n",
    "            peft_model.save_pretrained('./model_chkpt/lora_adaptor/', save_adapter=True, save_config=True)\n",
    "            \n",
    "        if step >= max_steps:\n",
    "            print(\"Training finished.\")\n",
    "            break\n",
    "            \n",
    "        wandb.log({\"step\": step, \"train_loss\": loss.item()})\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d4b60-d49a-484a-8c87-cd7bf356cbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
