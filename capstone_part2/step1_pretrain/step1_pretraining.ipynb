{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "196cb1c8-72e3-4651-9a7a-5cd8b70bcdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob2\n",
    "import torch\n",
    "from step1_network import CLIPPhi2Model, train_model, frange_cycle_linear\n",
    "from step1_dataset import collate_fn, llavadataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import wandb\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8103b76-5435-4f19-8d1b-bee880404519",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"coco_dataset_pickle\", \"rb\") as fp:   # Unpickling\n",
    "    coco_unpickle = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b01a16-1989-460e-bce4-be20b1cefd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3f307b1685494da6c4f99448a463ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- configuration_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- modeling_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff70652f64c4c7e8673e854d01f566c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921a090fa7884e779611ba28ff860229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "clip_model_name  = \"openai/clip-vit-base-patch32\"\n",
    "phi_model_name   = \"microsoft/phi-2\"\n",
    "train_batch_size = 2\n",
    "val_batch_size   = 4\n",
    "device     = 'cuda'\n",
    "tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
    "\n",
    "# model\n",
    "MModalGPT        = CLIPPhi2Model().to(device)\n",
    "max_steps        = 100000\n",
    "model_save_step  = 1000\n",
    "model_val_step   = 1000\n",
    "log_step         = 1000\n",
    "max_token_filter = 35 # memory management restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d056e4a3-a835-4dd9-8bb7-cb84e462ad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 532577 and validation size 59176\n",
      "Train size 532577 and validation size 59176\n"
     ]
    }
   ],
   "source": [
    "# data loaders\n",
    "train_dataloader = DataLoader(llavadataset(coco_unpickle, phi_model_name,clip_model_name,'train',tokenizer),\n",
    "                  collate_fn=collate_fn, batch_size=train_batch_size, num_workers = 10, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_dataloader   = DataLoader(llavadataset(coco_unpickle, phi_model_name,clip_model_name,'val',tokenizer),\n",
    "                  collate_fn=collate_fn, batch_size=val_batch_size, num_workers = 10, shuffle=True, pin_memory=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, MModalGPT.parameters()), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1db0d-7e18-46c7-9a70-748ad26cc4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msanjanatule\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20240126_221717-jvdy87ik</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sanjanatule/clip_phi2_project/runs/jvdy87ik' target=\"_blank\">step1_pretrain</a></strong> to <a href='https://wandb.ai/sanjanatule/clip_phi2_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sanjanatule/clip_phi2_project' target=\"_blank\">https://wandb.ai/sanjanatule/clip_phi2_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sanjanatule/clip_phi2_project/runs/jvdy87ik' target=\"_blank\">https://wandb.ai/sanjanatule/clip_phi2_project/runs/jvdy87ik</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n",
      "Step 0/100000: Avg Running Loss = 3.715071678161621\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " A table topped with wine glasses full of wine.<|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A table with a variety of food and drinks on it a. a. a. a.<|endoftext|> \n",
      "1 - Target captions:\n",
      " A white computer monitor on a cluttered wooden table.  \n",
      "1 - predicted_captions:\n",
      " A laptop on a desk with a desk and a desk. a.. a. a.<|endoftext|> \n",
      "2 - Target captions:\n",
      " a sleek white imac mouse made by Apple<|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A laptop on a table with a a a a a. a. a. a. a<|endoftext|> \n",
      "3 - Target captions:\n",
      " Remote and various clutter on sofa chair of mattress.<|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A bed with a laptop on it a bed a. a. a. a. a.<|endoftext|> \n",
      "Step 1000/100000: Avg Running Loss = 4.658684540987014\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " a close up of a cat near a vase with flowers  \n",
      "0 - predicted_captions:\n",
      " A cat sitting on a shelf of a a a. a. a... a.<|endoftext|> \n",
      "1 - Target captions:\n",
      " A vase holding various pink and red flowers.<|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A bunch of flowers and a bunch of plants on a table in a room. a table a<|endoftext|> \n",
      "2 - Target captions:\n",
      " Seniors enjoying a wine tasting at a vineyard<|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A group of people standing in front of a a a a. a a a a a a<|endoftext|> \n",
      "3 - Target captions:\n",
      " A picture of a scene in a baseball game.<|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A baseball player is holding a bat and a ball in a... field. field.<|endoftext|> \n",
      "Step 2000/100000: Avg Running Loss = 4.616224375963211\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " A man is wearing a sign around his neck that says \"BILLSHRINK\".  \n",
      "0 - predicted_captions:\n",
      " A man is holding a phone and a a a. a. a. a. a.<|endoftext|> \n",
      "1 - Target captions:\n",
      " A series of soda bottles made into vases for pink flowers.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A vase of flowers sits on a table in a. a. a... a<|endoftext|> \n",
      "2 - Target captions:\n",
      " A woman holds her phone up to show something.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A group of girls sitting on a table with a. a.......<|endoftext|> \n",
      "3 - Target captions:\n",
      " An old open refrigerator in a dilapidated room.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A kitchen with a stove and a refrigerator...........<|endoftext|> \n",
      "Step 3000/100000: Avg Running Loss = 4.556091197013855\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " A person with a helmet wearing red flying a kite.<|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A man is flying a kite in the sky... a. a. a.<|endoftext|> \n",
      "1 - Target captions:\n",
      " a person laying on a couch covered by a sheet <|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A man is sitting at a table with a a a a a a a a a a a<|endoftext|> \n",
      "2 - Target captions:\n",
      " There is a clock on the wall above the ticket office.<|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A clock tower in a city street............<|endoftext|> \n",
      "3 - Target captions:\n",
      " a tray of food with eggs sausage and a glass of orange juice  \n",
      "3 - predicted_captions:\n",
      " A table with a plate of food on it a a. a. a....<|endoftext|> \n",
      "Step 4000/100000: Avg Running Loss = 4.504375534772873\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " A kitchenette with white counter tops a small stove, and a microwave above the stove.   \n",
      "0 - predicted_captions:\n",
      " A kitchen with a sink, stove, and a refrigerator........<|endoftext|> \n",
      "1 - Target captions:\n",
      " Professional baseball batter and catcher during a game<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A baseball player is holding a bat and a ball.........<|endoftext|> \n",
      "2 - Target captions:\n",
      " A person hitting a tennis ball on a court.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A man is playing tennis on a court. a. a. a... a.<|endoftext|> \n",
      "3 - Target captions:\n",
      " A man sitting in a car that has a lot of CB equipment.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A man is sitting in the front of a car with a.. a. a. a<|endoftext|> \n",
      "Step 5000/100000: Avg Running Loss = 4.513220928907394\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " A boy is holding a teddy bear figure.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A boy is holding a stuffed animal in his. a........<|endoftext|> \n",
      "1 - Target captions:\n",
      " A woman waiting in a line on her cell phone at night.   \n",
      "1 - predicted_captions:\n",
      " A man is standing in front of a building with a. a. a. a. a<|endoftext|> \n",
      "2 - Target captions:\n",
      " A laptop is connected to a computer monitor.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A computer and a mouse on a desk. a.........<|endoftext|> \n",
      "3 - Target captions:\n",
      " The bear is sitting on a box between two devices<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A stuffed teddy bear sitting on a table a. a. a. a...<|endoftext|> \n",
      "Step 6000/100000: Avg Running Loss = 4.422216621160508\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " Blueberries, potatoes and meatloaf are on a paper plate.  \n",
      "0 - predicted_captions:\n",
      " A plate of food with a piece of a a. a. a. a. a.<|endoftext|> \n",
      "1 - Target captions:\n",
      " A smiling man and woman hold a cell phone and computer cable.<|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A man is eating a pizza while sitting on a a computer. a. a. a.<|endoftext|> \n",
      "2 - Target captions:\n",
      " A large desk with a laptop and a computer on it.<|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A desk with a computer and a a a. a.... a. a.<|endoftext|> \n",
      "3 - Target captions:\n",
      " A man holding a tennis racket is hitting a ball.<|endoftext|><|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A man is playing tennis with a ball a. a. a... a. a<|endoftext|> \n",
      "Step 7000/100000: Avg Running Loss = 4.458175351858139\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " A woman wearing a very colorful costume while checking her phone.<|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A woman is holding a bunch of flowers in a. a. a. a. a.<|endoftext|> \n",
      "1 - Target captions:\n",
      " There are a lot of people walking across the plaza.<|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A man with a large umbrella walking in a a. a. a. a. a.<|endoftext|> \n",
      "2 - Target captions:\n",
      " Two young men playing Wii in a messy room.<|endoftext|><|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A man is sitting on a couch playing a video game a. a. a. a.<|endoftext|> \n",
      "3 - Target captions:\n",
      " Three vases are on display on the floor of a house.  \n",
      "3 - predicted_captions:\n",
      " A bunch of flowers are sitting on a a a a a a a a a a a a<|endoftext|> \n",
      "Step 8000/100000: Avg Running Loss = 4.4054744687080385\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " A room with a couch, a table and a chair.<|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A living room with a couch, a table, and a a. a. a. a<|endoftext|> \n",
      "1 - Target captions:\n",
      " Several animal figurines are displayed around a Chinese vase.<|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A display of a variety of stuffed animals. a. a. a. a. a.<|endoftext|> \n",
      "2 - Target captions:\n",
      " a man and woman look at a laptop while holding a young child  \n",
      "2 - predicted_captions:\n",
      " A man and a woman sitting in front of a a a a a a a a a a<|endoftext|> \n",
      "3 - Target captions:\n",
      " A laptop is siitting on a wooden surface by a telephone<|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A computer with a mouse and keyboard on it. a. a. a. a. a<|endoftext|> \n",
      "Step 9000/100000: Avg Running Loss = 4.395979427337647\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " An oven with multiple racks in a kitchen.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A kitchen with a refrigerator, oven, and..... and....<|endoftext|> \n",
      "1 - Target captions:\n",
      " A plate containing some eggs and some mushrooms on a slice of bread.  \n",
      "1 - predicted_captions:\n",
      " A plate of food with a piece of meat and a. a. a. a. a<|endoftext|> \n",
      "2 - Target captions:\n",
      " A shelf with a vase and bird decorations on it.<|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A bowl of water with a fish and a..........<|endoftext|> \n",
      "3 - Target captions:\n",
      " A batter with batting helmet getting ready to hit.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A baseball player is holding a baseball bat in a.........<|endoftext|> \n",
      "Step 10000/100000: Avg Running Loss = 4.392810859918594\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " A mini-fridge and microwave inside of a hotel room.<|endoftext|><|endoftext|><|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A kitchen with a microwave and a microwave. a. a. a. a. a.<|endoftext|> \n",
      "1 - Target captions:\n",
      " There are three teddy bears sitting next to each other.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A teddy bear sitting on a bed with a a........<|endoftext|> \n",
      "2 - Target captions:\n",
      " People using cell phones sitting around a wooden table.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A man is sitting at a table with a a a a a a a a a a a<|endoftext|> \n",
      "3 - Target captions:\n",
      " a close up of a hair dryer on the ground in front of a mirror  \n",
      "3 - predicted_captions:\n",
      " A computer sitting on a desk with a a..........<|endoftext|> \n",
      "Step 11000/100000: Avg Running Loss = 4.381994881391526\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " The young base ball player has swung the bat.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A man is playing with a baseball bat a. a. a. a. a. a<|endoftext|> \n",
      "1 - Target captions:\n",
      " A lglass vase with two orange tulips on a wood table.<|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A vase of flowers on a table table table table table table table table table table table table<|endoftext|> \n",
      "2 - Target captions:\n",
      " A pair of scissors, a crochet hook and a sewing needle are ready to craft.  \n",
      "2 - predicted_captions:\n",
      " A set of knives and a cutting board on a.........<|endoftext|> \n",
      "3 - Target captions:\n",
      " Two people flying a butterfly kite on the beach.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A man flying a kite on the beach a beach. the. the. the. the<|endoftext|> \n",
      "Step 12000/100000: Avg Running Loss = 4.342394594669342\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " THERE IS A SLICE OF PIZZA THAT IS ON THE TABLE   \n",
      "0 - predicted_captions:\n",
      " A pizza with a picture of a on on on. on. on. on. on.<|endoftext|> \n",
      "1 - Target captions:\n",
      " A all building with a crowd of people at the bottom of it.<|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A large building with a large sign on it... street. street. street. street<|endoftext|> \n",
      "2 - Target captions:\n",
      " A woman in sunglasses is talking on the phone.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A woman holding a phone in her hand a..........<|endoftext|> \n",
      "3 - Target captions:\n",
      " A guy holding a remote up to a very nice looking TV.<|endoftext|><|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A man and a woman sitting in front of a a a a a a a a a a<|endoftext|> \n",
      "Step 13000/100000: Avg Running Loss = 4.358855091571808\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " A group of people are having a meal on a wooden park table.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A group of people are eating food in a..........<|endoftext|> \n",
      "1 - Target captions:\n",
      " A young woman dressed up as a rapper drinking a beer.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A man is holding a phone in his hand..........<|endoftext|> \n",
      "2 - Target captions:\n",
      " Antique phone, fan and clock among other things beside the latest technology of communication.   \n",
      "2 - predicted_captions:\n",
      " A desk with a computer and a laptop on it. and. a. a. a.<|endoftext|> \n",
      "3 - Target captions:\n",
      " Two girls are having a lunch with noodles.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A woman is eating at a table with a and a. a. a. a. a<|endoftext|> \n",
      "Step 14000/100000: Avg Running Loss = 4.3658397834301\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " a man sitting in the yard checking out a kite <|endoftext|><|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A man is holding a kite in a a a. a......<|endoftext|> \n",
      "1 - Target captions:\n",
      " two boys playing with a phone next to a man<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A man and a woman with a child a phone phone........<|endoftext|> \n",
      "2 - Target captions:\n",
      " A stainless steel refrigerator and freezer in a house<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A bathroom with a toilet and a............<|endoftext|> \n",
      "3 - Target captions:\n",
      " A black tray with bowl of soup, cup of coffee and meat.  \n",
      "3 - predicted_captions:\n",
      " A table with a plate of food and a a and a. and a. and a.<|endoftext|> \n",
      "Step 15000/100000: Avg Running Loss = 4.297978925228119\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " Little girl sitting on a step next to a brown teddy bear.   \n",
      "0 - predicted_captions:\n",
      " A little girl is sitting on the floor with a t t t t t t t t t<|endoftext|> \n",
      "1 - Target captions:\n",
      " people cooking pancakes with two different cookers on a table<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A group of people are sitting at a table a table. table. table. table. table<|endoftext|> \n",
      "2 - Target captions:\n",
      " The two girls are enjoying their game of dizzy bat.<|endoftext|><|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A man and a woman playing baseball on a field.........<|endoftext|> \n",
      "3 - Target captions:\n",
      " Someone serving a plate of food onto another's plate.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A man is cutting up a piece of meat on a. a. a. a. a<|endoftext|> \n",
      "Step 16000/100000: Avg Running Loss = 4.324153872966766\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " A laptop on top of a table in a room.<|endoftext|><|endoftext|><|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A laptop is on a table next to a fireplace.........<|endoftext|> \n",
      "1 - Target captions:\n",
      " A farm yard with several farm buildings and a fence. <|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A barn with a few cows in the grass..........<|endoftext|> \n",
      "2 - Target captions:\n",
      " A plate with pizza on it by a wine glass on a table.  \n",
      "2 - predicted_captions:\n",
      " A plate of pizza and a glass of on a table.... table...<|endoftext|> \n",
      "3 - Target captions:\n",
      " Two people standing on the beach flying a kite.<|endoftext|><|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A man flying a kite on a beach beach....... the.<|endoftext|> \n",
      "Step 17000/100000: Avg Running Loss = 4.340470662117005\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " A woman standing in a store next to a man.<|endoftext|><|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A group of people standing in a line of a. a. phone.....<|endoftext|> \n",
      "1 - Target captions:\n",
      " Items clutter a counter in front of a glass door and window.  \n",
      "1 - predicted_captions:\n",
      " A kitchen with a refrigerator, microwave, and.... and.... and<|endoftext|> \n",
      "2 - Target captions:\n",
      " a tennis player swinging a racket at a ball<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A man holding a tennis racket and a ball..........<|endoftext|> \n",
      "3 - Target captions:\n",
      " A dog peers out from the inside of a microwave.<|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A dog is sitting on a television remote...........<|endoftext|> \n",
      "Step 18000/100000: Avg Running Loss = 4.316782158613205\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " two vases constructed with furry materials in front of a plant<|endoftext|><|endoftext|><|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A small, white, round, and a a a a a a a a a a a<|endoftext|> \n",
      "1 - Target captions:\n",
      " A brown couch with four pillows and a dog laying next to it.  \n",
      "1 - predicted_captions:\n",
      " A living room with a couch, a and a and a and a and a and a and<|endoftext|> \n",
      "2 - Target captions:\n",
      " The baseball player swings the bat at the ball<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A baseball player is standing on the field. a. a. a. a. a.<|endoftext|> \n",
      "3 - Target captions:\n",
      " A woman at a table with plates of salad and pizza.  <|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A woman is eating a plate of food and a a a. a. a. a.<|endoftext|> \n",
      "Step 19000/100000: Avg Running Loss = 4.284749105930328\n",
      "Batch skipped as captions too long.\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " We are looking up a a tall clock tower.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A tall building with a large tower in the sky. sky. sky. sky. sky.<|endoftext|> \n",
      "1 - Target captions:\n",
      " Plates of food are seen behind a glass of wine.<|endoftext|><|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A glass of wine is on a table a a a plate a a plate a a a plate<|endoftext|> \n",
      "2 - Target captions:\n",
      " A copious amount of food are served up in the kitchen wares.  \n",
      "2 - predicted_captions:\n",
      " A plate of food with a variety of vegetables and.... and.. and.<|endoftext|> \n",
      "3 - Target captions:\n",
      " A pink teddy bear is straddled on a fence.<|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A teddy bear sitting on a wooden bench..........<|endoftext|> \n",
      "Step 20000/100000: Avg Running Loss = 4.269168333530426\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " The sparsely-decorated desk holds only a laptop, keyboard, mouse, and a note holder.  \n",
      "0 - predicted_captions:\n",
      " A computer on a desk with a keyboard and a.........<|endoftext|> \n",
      "1 - Target captions:\n",
      " There are a number of electronics set on a wooden table.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A computer monitor and keyboard on a desk...........<|endoftext|> \n",
      "2 - Target captions:\n",
      " a small boy is playing a video game<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A young man is playing a video game on a a. a. a. a. a<|endoftext|> \n",
      "3 - Target captions:\n",
      " The tower on the building has a clock displayed. <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A clock tower with a clock on it a. a. a. a. a. a<|endoftext|> \n",
      "Step 21000/100000: Avg Running Loss = 4.3013080427646635\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " Two women and a man at a laptop are in a business meeting at lunch time.   \n",
      "0 - predicted_captions:\n",
      " A man and a woman sitting at a a a a laptop. a. a. a.<|endoftext|> \n",
      "1 - Target captions:\n",
      " Several glasses of wine standing on a table.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A table with a number of glasses of wine on a. a. a. a. a<|endoftext|> \n",
      "2 - Target captions:\n",
      " Pitcher just throwing ball to another player during a game <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A man playing baseball with a group of people.... field.....<|endoftext|> \n",
      "3 - Target captions:\n",
      " A man brushes his teeth with an electronic toothbrush while he looks into the mirror.<|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A man and a woman are both holding a a a tooth. a. a. a.<|endoftext|> \n",
      "Step 22000/100000: Avg Running Loss = 4.2894068381786346\n",
      "Batch skipped as captions too long.\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " Young male batter at home plate with spectator nearby.<|endoftext|><|endoftext|><|endoftext|>  \n",
      "0 - predicted_captions:\n",
      " A man is holding a baseball bat and a a. a. a. a. a.<|endoftext|> \n",
      "1 - Target captions:\n",
      " A person is sitting at a table full of glasses.<|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A table with a vase of flowers and a a a a glasses a. a. a<|endoftext|> \n",
      "2 - Target captions:\n",
      " Two girls and a man are pouring wine.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A man and a woman are holding a a a a. a. a. a. a<|endoftext|> \n",
      "3 - Target captions:\n",
      " A tennis player serves on a court next to a parking lot.  \n",
      "3 - predicted_captions:\n",
      " A woman is playing tennis on a court... a. a. a. a.<|endoftext|> \n",
      "Step 23000/100000: Avg Running Loss = 4.230409128427506\n",
      "Batch skipped as captions too long.\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"clip_phi2_project\", name=\"step1_pretrain\")\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "train_model(MModalGPT, train_dataloader, val_dataloader, optimizer, device, max_steps,model_save_step,model_val_step,log_step,max_token_filter,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f16870-145b-4752-ad0e-576152d256bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
