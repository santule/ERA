{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_uwOFyPO7sne","executionInfo":{"status":"ok","timestamp":1699904415591,"user_tz":-60,"elapsed":29279,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}},"outputId":"637d15c6-d0e8-48ea-e987-820207e4f447"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["! cp /content/drive/MyDrive/ERAv1/S22/session22/iter-010915-ckpt.pth.zip .\n","! unzip iter-010915-ckpt.pth.zip\n","\n","\n","! cp /content/drive/MyDrive/ERAv1/S22/session22/data1.zip .\n","! unzip data1.zip"],"metadata":{"id":"ZMWuyV4EFC10"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install lightning sentencepiece -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r7jiOENk8Vct","executionInfo":{"status":"ok","timestamp":1699904836106,"user_tz":-60,"elapsed":13938,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}},"outputId":"18321c29-3d94-40f8-857e-e379ffb24b1c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["@torch.inference_mode()\n","def generate(\n","    model: GPT,\n","    idx: torch.Tensor,\n","    max_returned_tokens: int,\n","    *,\n","    temperature: float = 1.0,\n","    top_k:int = None,\n","    eos_id:int = None,\n",") -> torch.Tensor:\n","    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n","\n","    The implementation of this function is modified from A. Karpathy's nanoGPT.\n","\n","    Args:\n","        model: The model to use.\n","        idx: Tensor of shape (T) with indices of the prompt sequence.\n","        max_returned_tokens: The maximum number of tokens to return (given plus generated).\n","        temperature: Scales the predicted logits by 1 / temperature.\n","        top_k: If specified, only sample among the tokens with the k highest probabilities.\n","        eos_id: If specified, stop generating any more token once the <eos> token is triggered.\n","    \"\"\"\n","    T = idx.size(0)\n","    assert max_returned_tokens > T\n","    if model.max_seq_length < max_returned_tokens - 1:\n","        # rolling the kv cache based on the `input_pos` value would be necessary. However, doing so would introduce a\n","        # data dependency on the `input_pos` tensor and impact model compilation. Since this setting is uncommon, we do\n","        # not support it to avoid negatively impacting the overall speed\n","        raise NotImplementedError(f\"max_seq_length {model.max_seq_length} needs to be >= {max_returned_tokens - 1}\")\n","\n","    device, dtype = idx.device, idx.dtype\n","    # create an empty tensor of the expected final shape and fill in the current tokens\n","    empty = torch.empty(max_returned_tokens, dtype=dtype, device=device)\n","    empty[:T] = idx\n","    idx = empty\n","    input_pos = torch.arange(0, T, device=device)\n","\n","    # generate up to a fixed number of tokens\n","    for _ in range(max_returned_tokens - T):\n","        x = idx.index_select(0, input_pos).view(1, -1)\n","\n","        # forward\n","        logits = model(x, input_pos)\n","        logits = logits[0, -1] / temperature\n","\n","        # optionally crop the logits to only the top k options\n","        if top_k is not None:\n","            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n","            logits = torch.where(logits < v[[-1]], -float(\"Inf\"), logits)\n","\n","        probs = torch.nn.functional.softmax(logits, dim=-1)\n","        idx_next = torch.multinomial(probs, num_samples=1).to(dtype=dtype)\n","\n","        # advance\n","        input_pos = input_pos[-1:] + 1\n","\n","        # concatenate the new generation\n","        idx = idx.index_copy(0, input_pos, idx_next)\n","\n","        # if <eos> token is triggered, return the output (stop generation)\n","        if idx_next == eos_id:\n","            return idx[:input_pos]  # include the EOS token\n","\n","    return idx\n"],"metadata":{"id":"Q6pphpTSUuC0","executionInfo":{"status":"ok","timestamp":1699914374918,"user_tz":-60,"elapsed":522,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["import time\n","import torch\n","import lightning as L\n","from torch.utils.data import DataLoader\n","from lightning.fabric.loggers import CSVLogger\n","from lightning.fabric.strategies import FSDPStrategy"],"metadata":{"id":"S-wd22H-Bvi2","executionInfo":{"status":"ok","timestamp":1699914092167,"user_tz":-60,"elapsed":371,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["from tsai_gpt.model import GPT, Block, Config\n","from tsai_gpt.tokenizer import Tokenizer\n","from tsai_gpt.packed_dataset import CombinedDataset, PackedDataset\n","from tsai_gpt.speed_monitor import SpeedMonitorBase, estimate_flops, measure_flops\n","from tsai_gpt.speed_monitor import SpeedMonitorFabric as SpeedMonitor\n","from tsai_gpt.utils import chunked_cross_entropy, get_default_supported_precision, num_parameters, load_checkpoint, gptq_quantization"],"metadata":{"id":"BDeqgTVGB6lv","executionInfo":{"status":"ok","timestamp":1699914168558,"user_tz":-60,"elapsed":1452,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["model_name = \"pythia-160m\"\n","name = \"redpajama\"\n","save_interval = 1000\n","eval_interval = 1000\n","eval_iters = 100\n","log_interval = 100"],"metadata":{"id":"qv2Y2DwRB_LV","executionInfo":{"status":"ok","timestamp":1699914172921,"user_tz":-60,"elapsed":331,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","learning_rate = 6e-3\n","batch_size = 32\n","micro_batch_size = 8\n","gradient_accumulation_steps = batch_size // micro_batch_size\n","assert gradient_accumulation_steps > 0\n","#max_iters = 600000  # num_epochs * (epoch_size // micro_batch_size) // devices\n","max_iters = 15000\n","weight_decay = 1e-1\n","beta1 = 0.9\n","beta2 = 0.95\n","grad_clip = 1.0\n","decay_lr = True\n","warmup_iters = 2000\n","lr_decay_iters = max_iters\n","min_lr = 6e-6"],"metadata":{"id":"LG1uj_TeCVhr","executionInfo":{"status":"ok","timestamp":1699817820725,"user_tz":-60,"elapsed":2,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","\n","def _init_weights(module: nn.Module) -> None:\n","        \"\"\"Meant to be used with `gpt.apply(gpt._init_weights)`.\"\"\"\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n"],"metadata":{"id":"HsmmmL6JCmWS","executionInfo":{"status":"ok","timestamp":1699914177077,"user_tz":-60,"elapsed":3,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["config = Config.from_name(model_name)\n","model = GPT(config)"],"metadata":{"id":"24iFJivjDx4U","executionInfo":{"status":"ok","timestamp":1699914183439,"user_tz":-60,"elapsed":3358,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"d7u739jZEGt9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next(model.parameters()).sum() #-25 -2 -860"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5dbVvWeBEGwk","executionInfo":{"status":"ok","timestamp":1699821040052,"user_tz":-60,"elapsed":581,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}},"outputId":"22eae69d-37c1-4701-f234-94b951afe7d4"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-86016., dtype=torch.bfloat16, grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":[],"metadata":{"id":"npWJrKGPPZ8p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.apply(_init_weights)"],"metadata":{"id":"OZlQfFjBEG0B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict"],"metadata":{"id":"7qao3hArEl8m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path"],"metadata":{"id":"Vj2uOZpgGjMx","executionInfo":{"status":"ok","timestamp":1699914265467,"user_tz":-60,"elapsed":584,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["prompt= \"Hello, my name is\"\n","num_samples = 1\n","max_new_tokens = 50\n","top_k = 200\n","temperature = 0.8\n","checkpoint_dir = Path(\"/content/iter-010915-ckpt.pth\")\n","quantize = None\n","strategy = \"auto\"\n","devices = 1\n","precision = None"],"metadata":{"id":"bTU6o6BoFFz1","executionInfo":{"status":"ok","timestamp":1699914266805,"user_tz":-60,"elapsed":2,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["import sys\n","\n","precision = get_default_supported_precision(training=False)\n","plugins = None\n","fabric = L.Fabric(devices=devices, precision=precision, strategy=strategy, plugins=plugins)\n","fabric.launch()\n","fabric.print(f\"Loading model {str(checkpoint_dir)!r} with {config.__dict__}\", file=sys.stderr)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Py5SksKnNEmg","executionInfo":{"status":"ok","timestamp":1699914270524,"user_tz":-60,"elapsed":408,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}},"outputId":"e67d09d6-50b0-4f6f-9fd9-81f40c663289"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":["Loading model '/content/iter-010915-ckpt.pth' with {'name': 'pythia-160m', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n"]}]},{"cell_type":"code","source":["with fabric.init_module(empty_init=True), gptq_quantization(quantize==\"gptq.int4\"):\n","    model = GPT(config)\n","\n","t0 = time.perf_counter()\n","fabric.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\", file=sys.stderr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ct7N-wrXHU-g","executionInfo":{"status":"ok","timestamp":1699914273603,"user_tz":-60,"elapsed":9,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}},"outputId":"9ae0cde9-5c75-43e5-fcbc-9186547da8d3"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stderr","text":["Time to instantiate model: 0.00 seconds.\n"]}]},{"cell_type":"code","source":["model.eval()\n","model = fabric.setup_module(model)\n","\n","t0 = time.perf_counter()\n","load_checkpoint(fabric, model, checkpoint_dir)\n","fabric.print(f\"Time to load the model weights: {time.perf_counter() - t0:.02f} seconds.\", file=sys.stderr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EyocNBbFOBV2","executionInfo":{"status":"ok","timestamp":1699914285829,"user_tz":-60,"elapsed":8834,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}},"outputId":"b867e58a-d954-4670-ce69-4c6df912e9d5"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stderr","text":["Time to load the model weights: 8.34 seconds.\n"]}]},{"cell_type":"code","source":["tokenizer = Tokenizer(Path('/content/tokenizer'))\n","encoded = tokenizer.encode(prompt, device=fabric.device)\n","prompt_length = encoded.size(0)\n","max_returned_tokens = prompt_length + max_new_tokens"],"metadata":{"id":"Cn31RtGjPaz5","executionInfo":{"status":"ok","timestamp":1699914341771,"user_tz":-60,"elapsed":593,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["with fabric.init_tensor():\n","    # set the max_seq_length to limit the memory usage to what we need\n","    model.max_seq_length = max_returned_tokens"],"metadata":{"id":"k7kK_OMyP_kI","executionInfo":{"status":"ok","timestamp":1699914345485,"user_tz":-60,"elapsed":404,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["for i in range(num_samples):\n","    with fabric.init_tensor():\n","        # enable the kv cache\n","        model.set_kv_cache(batch_size=1)\n","\n","    t0 = time.perf_counter()\n","    y = generate(model, encoded, max_returned_tokens, temperature=temperature, top_k=top_k)\n","    t = time.perf_counter() - t0\n","\n","    fabric.print(tokenizer.decode(y))\n","    tokens_generated = y.size(0) - prompt_length\n","    fabric.print(\n","        f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gkTcFPPjR9tb","executionInfo":{"status":"ok","timestamp":1699823427254,"user_tz":-60,"elapsed":11276,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}},"outputId":"772a3bda-60f8-48cd-f2dd-210bdf4f888d"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, my name is Red Hat (aka \"Dentals\") and I think we got born here.\n","I agree with that same name, and I still get a name in Red Hat!\n","I would love their name and to the name of the Red Hat\n"]},{"output_type":"stream","name":"stderr","text":["Time for inference 1: 10.63 sec total, 4.70 tokens/sec\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"L3hl1rphOkTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"raPLIAfjOkXB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install gradio"],"metadata":{"id":"UV9xpnfOOkaI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UgKspT6-OkdF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7jA4vu_6OkgK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import torch\n","from torch import nn\n","import lightning.pytorch as pl\n","from torch.nn import functional as F\n","\n","device     = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","def generate_dialogue(input_text, temperature, max_tokens, top_k):\n","    encoded = tokenizer.encode(input_text, device=fabric.device)\n","    max_returned_tokens = encoded.size(0) + max_tokens\n","\n","\n","    with fabric.init_tensor():\n","        # set the max_seq_length to limit the memory usage to what we need\n","        model.max_seq_length = max_returned_tokens\n","\n","\n","    with fabric.init_tensor():\n","        model.set_kv_cache(batch_size=1)\n","\n","    y = generate(model, encoded, max_returned_tokens, temperature=temperature, top_k=top_k)\n","\n","    return(tokenizer.decode(y))\n","\n","\n","\n","\n","example_text = [\n","    \"In a galaxy far, far away, an intergalactic council convenes to discuss the rising cost of lightsaber batteries. Among them is an unlikely representative: a droid with a penchant for economics...\",\n","    \"As Sherlock Holmes and Dr. Watson enter the world of social media influencers, they find their first case: the mysterious disappearance of a famous TikTok star's like button.\",\n","    \"In the midst of a zombie apocalypse, a group of survivors discovers a library with every book intact except for cookbooks. Their leader, a former TV chef, decides to write the ultimate survival recipe book titled...\",\n","    \"A time traveler accidentally attends Shakespeare's first play, but instead of a quill, she hands him a smartphone with autocorrect. The resulting play is...\",\n","    \"Amidst the chaos of a Hogwarts School reunion, a magical mishap swaps the voices of Professors Dumbledore and Snape, leading to an unexpected duet in the Great Hall that goes viral in the wizarding world.\"\n","]\n","\n","examples = [\n","             [\n","                example_text[i],\n","                round(random.uniform(0,1), 1),\n","                int(random.uniform(50,200)),\n","                int(random.uniform(100,300))] for i,x in enumerate(example_text)\n","           ]"],"metadata":{"id":"yKC1UF0kOkix","executionInfo":{"status":"ok","timestamp":1699914066644,"user_tz":-60,"elapsed":348,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["\n","import gradio as gr\n","import torch\n","from torch import nn\n","import lightning.pytorch as pl\n","from torch.nn import functional as F\n","\n","\n","HTML_TEMPLATE = \"\"\"\n","<style>\n","\n","    #app-header {\n","        text-align: center;\n","        background: rgba(255, 255, 255, 0.3); /* Semi-transparent white */\n","        padding: 20px;\n","        border-radius: 10px;\n","        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n","        position: relative; /* To position the artifacts */\n","    }\n","    #app-header h1 {\n","        color: #FF0000;\n","        font-size: 2em;\n","        margin-bottom: 10px;\n","    }\n","    .concept {\n","        position: relative;\n","        transition: transform 0.3s;\n","    }\n","    .concept:hover {\n","        transform: scale(1.1);\n","    }\n","    .concept img {\n","        width: 100px;\n","        border-radius: 10px;\n","        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n","    }\n","    .concept-description {\n","        position: absolute;\n","        bottom: -30px;\n","        left: 50%;\n","        transform: translateX(-50%);\n","        background-color: #4CAF50;\n","        color: white;\n","        padding: 5px 10px;\n","        border-radius: 5px;\n","        opacity: 0;\n","        transition: opacity 0.3s;\n","    }\n","    .concept:hover .concept-description {\n","        opacity: 1;\n","    }\n","    /* Artifacts */\n","\n","</style>\n","<div id=\"app-header\">\n","    <!-- Artifacts -->\n","    <div class=\"artifact large\"></div>\n","    <div class=\"artifact large\"></div>\n","    <div class=\"artifact large\"></div>\n","    <div class=\"artifact large\"></div>\n","    <!-- Content -->\n","    <h1>GPT NEXT WORD GENERATOR</h1>\n","    <p>Generate dialogue for given some initial prompt for context.</p>\n","    <p>Model: GPT, Dataset: arxiv + book + cc, Parameter Count: 160M</p>\n","\"\"\"\n","\n","with gr.Blocks(theme=gr.themes.Glass(),css=\".gradio-container {background: url('file=https://github.com/Delve-ERAV1/Conditional-Diffusion/assets/11761529/1ff9d2e1-798f-442a-a1e2-386fdd35010a')}\") as interface:\n","    gr.HTML(value=HTML_TEMPLATE, show_label=False)\n","\n","    gr.Markdown(\"\")\n","    gr.Markdown(\"\")\n","    gr.Markdown(\"\")\n","\n","    gr.Markdown(\"\")\n","    gr.Markdown(\"\")\n","    gr.Markdown(\"\")\n","    gr.Markdown(\"\")\n","\n","    with gr.Row():\n","\n","        input_text = gr.Textbox(\n","            label=\"Input Text\",\n","            value=\"Enter your prompt here: This text will set the context for the AI's response.\"\n","        )\n","\n","        temperature_dropdown = gr.Slider(0, 1, value=0.8, label=\"Temperature\", info=\"Set the creativity level: Higher values produce more varied results, lower values generate more predictable text.\")\n","        top_k_dropdown = gr.Slider(50, 300, value=200, label=\"Top K\", info=\"Control the randomness: Limits the AI to consider only the top K most likely next words.\")\n","        max_new_tokens = gr.Slider(1, 100, value=50, label=\"Max Tokens\", info=\"Choose the length: This determines the maximum number of words the AI will generate.\")\n","\n","\n","        outputs = gr.Textbox(\n","            label=\"Generated Dialogue\"\n","        )\n","        inputs = [input_text, temperature_dropdown, top_k_dropdown, max_new_tokens]\n","\n","    with gr.Column():\n","        button = gr.Button(\"Generate\")\n","        button.click(generate_dialogue, inputs=inputs, outputs=outputs)\n","\n","    with gr.Row():\n","         gr.Examples(examples=examples, inputs=inputs, outputs=outputs, fn=generate_dialogue, cache_examples=True,)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qf_2-2hSOkl6","executionInfo":{"status":"ok","timestamp":1699923002970,"user_tz":-60,"elapsed":151599,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}},"outputId":"4fe2c6bc-4a3e-4db6-e381-4c8847a996a0"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Caching examples at: '/content/gradio_cached_examples/574'\n","Caching example 1/5\n","Caching example 2/5\n","Caching example 3/5\n","Caching example 4/5\n","Caching example 5/5\n"]}]},{"cell_type":"code","source":["interface.launch()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"id":"Qj9izLILQB5n","executionInfo":{"status":"ok","timestamp":1699923004616,"user_tz":-60,"elapsed":1686,"user":{"displayName":"Sijuade Oguntayo","userId":"13952043505652843687"}},"outputId":"d7add95a-c36d-4c50-fe68-fe825bc9cc34"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://d6a4765be993bd4045.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://d6a4765be993bd4045.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":54}]}]}