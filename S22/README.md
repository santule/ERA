# Training GPT from scratch using arxiv + book + cc.

## Key Features:
  1. Parameter Count: 160M
  2. Pythia-160M
  3. Training Loss 3.4297.
  4. Embedding Dimension: 768.
  5. Context Length: 2048.

## Hugging Face Spaces Link:

https://huggingface.co/spaces/sanjanatule/GPTNext


## Generate Dialogue for a character using the trained model - examples
![Screenshot 2023-10-26 at 10 11 16 pm](https://github.com/santule/ERA/assets/20509836/861c2961-dc71-4445-a1b3-b793067d6a62)

